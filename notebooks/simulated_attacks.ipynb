{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 1: Environment Setup & Dependencies**\n",
        "In this section, we configure the runtime environment, mount persistent storage (Google Drive), and authenticate with Hugging Face to access the required datasets."
      ],
      "metadata": {
        "id": "jYLI07ijHUQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Project Setup & Directory Initialization**\n",
        "We initialize the project structure by creating the necessary subdirectories for raw data, processed features, metadata, and model checkpoints."
      ],
      "metadata": {
        "id": "v8rVl7jrFzq1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dCNl237M_Kr"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/hindi_dfake/{raw,processed,metadata,scripts,checkpoints}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Environment Configuration**\n",
        "We mount Google Drive to ensure persistent storage and dynamically define the root paths (`ROOT_DIR`, `OUT_DIR`, etc.) to avoid hardcoding errors across different sessions."
      ],
      "metadata": {
        "id": "Zft52MN2Gr2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4eXmEHUZ1DP",
        "outputId": "4232c0b9-70d3-4191-eddf-37df6a0a6b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive is already mounted here → /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, glob\n",
        "MOUNT_PATH = \"/content/drive\" if os.path.isdir(\"/content/drive/MyDrive\") else \"/content/gdrive\"\n",
        "print(\"Drive is already mounted here →\", MOUNT_PATH)\n",
        "ROOT_DIR = f\"{MOUNT_PATH}/MyDrive/hindi_dfake\"\n",
        "OUT_DIR  = f\"{ROOT_DIR}/raw/real_clean/ivr\"\n",
        "CSV_PATH = f\"{ROOT_DIR}/metadata/master_real.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Hugging Face Authentication**\n",
        "Authentication is required to download the **IndicVoices-R** and **Common Voice** datasets directly from the Hugging Face Hub."
      ],
      "metadata": {
        "id": "zjux7tS5Gu4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "3d162393b84e41b8a1c6cc279e292827",
            "a8a36cdbf3704e29ad7c012fa5eff920",
            "0e55fd890f8f467a952ecac007bd64e8",
            "edfe3d2b8b2f49e6b7d127111c86b58b",
            "a0d2d38dfb0c4d43a3aec03152ded0d1",
            "20a24eecb7894fe4a835117815d4a804",
            "181e2d098b424310a55517eeb47737f0",
            "a7b814943b454759b4f207fa95011a1e",
            "377be24bb3504334adb534383019c715",
            "44b365d69b244d75bd13f24478a5b3c8",
            "0bf243b8172045c4ac92407d1b4c7c4e",
            "b49e60e1f4f544af853d4a889d75ac80",
            "db0eee444be14017a800f361d8642c27",
            "68e8dad5df2645869c8dd6675845d9bd",
            "ce058c87323b4a1887005d4d8bca6dbb",
            "176b79b7fb60414cb0dcdbfc068c612b",
            "479258ee28ad4e88a0829e1697681250",
            "b11cb96865cf4f659853ed4a7216591c",
            "35500233ad49478c8a7a38e2c1631911",
            "f5bf6da094f947ca9054667917cb3829"
          ]
        },
        "id": "0fQKZ7FJS69O",
        "outputId": "8ab170bf-7428-4316-c982-e7d273487ccb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d162393b84e41b8a1c6cc279e292827"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 2: Simulated Attacks**\n",
        "We apply codec compression (Opus 16k) and channel effects (Low-pass 8kHz) to a 40% subset of the data. This ensures the detector works on compressed, real-world audio."
      ],
      "metadata": {
        "id": "TmvNBr-2ByQF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLOSfVqx_GNr"
      },
      "source": [
        "### **1. Attack Dataset Selection**\n",
        "We select the files to be processed. You can adjust `REAL_FRACTION` and `FAKE_FRACTION` if you want to generate a smaller debug set first, or set them to `1.0` to process the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8lhQOM-vzsd",
        "outputId": "a144d00f-0347-46a0-8ac9-add850c68396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+   1.0s] Found /content/drive/MyDrive/hindi_dfake/metadata/master_real.csv (10.29 MB)\n",
            "[+   1.0s] Found /content/drive/MyDrive/hindi_dfake/metadata/master_fake.csv (9.65 MB)\n",
            "[+   1.0s] Reading CSVs...\n",
            "[+   2.5s] Loaded rows — real: 25,965 | fake: 23,731\n",
            "[+   2.5s] Filtering rows to target folders...\n",
            "[+   2.5s] After folder filter — real: 25,965 | fake: 23,731\n",
            "[+   2.5s] exists() on real...\n",
            "[+  40.9s] exists() checked: 10000/25965\n",
            "[+  43.8s] exists() checked: 20000/25965\n",
            "[+  45.7s] Real kept after exists(): 25,965\n",
            "[+  45.7s] exists() on fake...\n",
            "[+ 123.6s] exists() checked: 10000/23731\n",
            "[+ 126.4s] exists() checked: 20000/23731\n",
            "[+ 127.4s] Fake kept after exists(): 23,731\n",
            "[+ 127.5s] Selected inputs total: 49,696\n",
            "[+ 127.5s] Source breakdown:\n",
            "source\n",
            "fake_tts            23731\n",
            "real_ivr            11174\n",
            "real_commonvoice    10088\n",
            "real_indictts        4703\n",
            "[+ 127.5s] Preview paths:\n",
            "/content/drive/MyDrive/hindi_dfake/raw/real_clean/ivr/1a0b61275bc58baf.wav\n",
            "/content/drive/MyDrive/hindi_dfake/raw/real_clean/ivr/cf768300a41d3c2b.wav\n",
            "/content/drive/MyDrive/hindi_dfake/raw/real_clean/ivr/d50c992a776d9002.wav\n",
            "/content/drive/MyDrive/hindi_dfake/raw/real_clean/ivr/a6f9acfa4f328702.wav\n",
            "/content/drive/MyDrive/hindi_dfake/raw/real_clean/ivr/f96a2637798d415f.wav\n"
          ]
        }
      ],
      "source": [
        "# ===== Codec/Channel Attacks — DEBUG Selection (fast + verbose) =====\n",
        "# Real: raw/real_clean/{commonvoice, indictts, ivr}\n",
        "# Fake: raw/fake_tts\n",
        "\n",
        "# Paths\n",
        "ROOT = \"/content/drive/MyDrive/hindi_dfake\"\n",
        "META = f\"{ROOT}/metadata\"\n",
        "REAL_CSV   = f\"{META}/master_real.csv\"\n",
        "FAKE_CSV   = f\"{META}/master_fake.csv\"\n",
        "OUT_DIR    = f\"{ROOT}/processed/attacks\"   # next cells will write here\n",
        "\n",
        "# Include both classes\n",
        "INCLUDE_REAL = True\n",
        "INCLUDE_FAKE = True\n",
        "\n",
        "# Optional sampling to control runtime/storage\n",
        "REAL_FRACTION = 1.0   # try 0.10 first if you want to smoke-test\n",
        "FAKE_FRACTION = 1.0\n",
        "\n",
        "# Attack list is defined in the next cells; we just select rows here.\n",
        "\n",
        "# --- DEBUG switches ---\n",
        "PROBE_AUDIO_HEADERS = False   # set True to verify a small sample with sf.info\n",
        "PROBE_LIMIT_PER_SRC = 100     # header-probe at most this many files per source\n",
        "\n",
        "# --- deps ---\n",
        "!pip -q install pandas soundfile numpy > /dev/null\n",
        "\n",
        "import os, time, pandas as pd, numpy as np, soundfile as sf\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "start = time.time()\n",
        "def log(msg):\n",
        "    print(f\"[+{time.time()-start:6.1f}s] {msg}\", flush=True)\n",
        "\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 0) File presence\n",
        "for p in [REAL_CSV, FAKE_CSV]:\n",
        "    if os.path.exists(p):\n",
        "        log(f\"Found {p} ({os.path.getsize(p)/1e6:.2f} MB)\")\n",
        "    else:\n",
        "        log(f\"Missing: {p}\")\n",
        "\n",
        "# 1) Load minimal columns quickly\n",
        "USECOLS = [\"utt_id\",\"path\",\"duration\",\"speaker_id\",\"gender\",\"text\",\"label\"]\n",
        "log(\"Reading CSVs...\")\n",
        "dfr = pd.read_csv(REAL_CSV, usecols=[c for c in USECOLS if c != \"label\"], low_memory=False) if INCLUDE_REAL and os.path.exists(REAL_CSV) else pd.DataFrame(columns=USECOLS)\n",
        "dff = pd.read_csv(FAKE_CSV, usecols=USECOLS, low_memory=False) if INCLUDE_FAKE and os.path.exists(FAKE_CSV) else pd.DataFrame(columns=USECOLS)\n",
        "if \"label\" not in dfr.columns and len(dfr):\n",
        "    dfr[\"label\"] = \"real\"\n",
        "log(f\"Loaded rows — real: {len(dfr):,} | fake: {len(dff):,}\")\n",
        "\n",
        "# 2) Filter by folder (no regex groups; no warnings)\n",
        "def keep_real_folders(df):\n",
        "    p = df[\"path\"].astype(str)\n",
        "    mask = (\n",
        "        p.str.contains(\"/raw/real_clean/commonvoice\", regex=False, na=False) |\n",
        "        p.str.contains(\"/raw/real_clean/indictts\",   regex=False, na=False) |\n",
        "        p.str.contains(\"/raw/real_clean/ivr\",        regex=False, na=False)\n",
        "    )\n",
        "    return df[mask].copy()\n",
        "\n",
        "def keep_fake_folder(df):\n",
        "    p = df[\"path\"].astype(str)\n",
        "    mask = p.str.contains(\"/raw/fake_tts\", regex=False, na=False)\n",
        "    return df[mask].copy()\n",
        "\n",
        "log(\"Filtering rows to target folders...\")\n",
        "dfr = keep_real_folders(dfr) if len(dfr) else dfr\n",
        "dff = keep_fake_folder(dff)  if len(dff) else dff\n",
        "log(f\"After folder filter — real: {len(dfr):,} | fake: {len(dff):,}\")\n",
        "\n",
        "# 3) Fast existence check (no audio decode)\n",
        "def fast_exists(paths):\n",
        "    out = np.zeros(len(paths), dtype=bool)\n",
        "    for i, p in enumerate(paths):\n",
        "        out[i] = Path(p).exists()\n",
        "        if (i+1) % 10000 == 0:\n",
        "            log(f\"exists() checked: {i+1}/{len(paths)}\")\n",
        "    return out\n",
        "\n",
        "if len(dfr):\n",
        "    log(\"exists() on real...\")\n",
        "    m = fast_exists(dfr[\"path\"].astype(str).tolist())\n",
        "    dfr = dfr[m]\n",
        "    log(f\"Real kept after exists(): {len(dfr):,}\")\n",
        "if len(dff):\n",
        "    log(\"exists() on fake...\")\n",
        "    m = fast_exists(dff[\"path\"].astype(str).tolist())\n",
        "    dff = dff[m]\n",
        "    log(f\"Fake kept after exists(): {len(dff):,}\")\n",
        "\n",
        "# 4) Optional: header-only probe (sampled) — NOT full decode\n",
        "if PROBE_AUDIO_HEADERS:\n",
        "    log(\"Header-probing a small sample with soundfile.info()...\")\n",
        "    def probe_sample(df, name):\n",
        "        samp = df.sample(min(len(df), PROBE_LIMIT_PER_SRC), random_state=2025) if len(df) else df\n",
        "        sr_ct = Counter(); ch_ct = Counter()\n",
        "        bad = 0\n",
        "        for p in samp[\"path\"].astype(str):\n",
        "            try:\n",
        "                info = sf.info(p)\n",
        "                sr_ct[info.samplerate] += 1\n",
        "                ch_ct[info.channels]   += 1\n",
        "            except Exception:\n",
        "                bad += 1\n",
        "        log(f\"{name}: sample={len(samp)} | sr={dict(sr_ct)} | ch={dict(ch_ct)} | bad={bad}\")\n",
        "    probe_sample(dfr, \"real\")\n",
        "    probe_sample(dff, \"fake\")\n",
        "\n",
        "# 5) Optional sampling to control runtime/storage\n",
        "if 0 < REAL_FRACTION < 1.0 and len(dfr):\n",
        "    before = len(dfr); dfr = dfr.sample(frac=REAL_FRACTION, random_state=2025); log(f\"Sampled real: {before:,} -> {len(dfr):,}\")\n",
        "if 0 < FAKE_FRACTION < 1.0 and len(dff):\n",
        "    before = len(dff); dff = dff.sample(frac=FAKE_FRACTION, random_state=2025); log(f\"Sampled fake: {before:,} -> {len(dff):,}\")\n",
        "\n",
        "# 6) Tag sources for reporting\n",
        "def src_from_path(p: str) -> str:\n",
        "    if \"/raw/real_clean/commonvoice\" in p: return \"real_commonvoice\"\n",
        "    if \"/raw/real_clean/indictts\"   in p: return \"real_indictts\"\n",
        "    if \"/raw/real_clean/ivr\"        in p: return \"real_ivr\"\n",
        "    if \"/raw/fake_tts\"              in p: return \"fake_tts\"\n",
        "    return \"other\"\n",
        "\n",
        "if len(dfr): dfr[\"source\"] = dfr[\"path\"].astype(str).map(src_from_path)\n",
        "if len(dff): dff[\"source\"] = dff[\"path\"].astype(str).map(src_from_path)\n",
        "\n",
        "# 7) Combine and show a tiny preview\n",
        "df_all = pd.concat([dfr, dff], ignore_index=True)\n",
        "log(f\"Selected inputs total: {len(df_all):,}\")\n",
        "if len(df_all):\n",
        "    log(\"Source breakdown:\\n\" + df_all[\"source\"].value_counts().to_string())\n",
        "    log(\"Preview paths:\\n\" + \"\\n\".join(df_all[\"path\"].astype(str).head(5).tolist()))\n",
        "else:\n",
        "    log(\"No inputs selected — check your paths in master_* CSVs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doFTULB5EemW"
      },
      "source": [
        "### **2. Attack Generation**\n",
        "We apply **Opus compression** (16k) and **Low-pass filtering** (8kHz) to the selected subset (40%) of real and fake files. This process is parallelized and resumable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZA1Omp_yN9U",
        "outputId": "e3ad02fe-7d09-4671-cc18-7ed002b3be8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subset sizes by label/source:\n",
            "label     source          \n",
            "fake      fake_tts            8667\n",
            "real      real_commonvoice    4022\n",
            "          real_indictts       1833\n",
            "          real_ivr            4481\n",
            "tts_edge  fake_tts             900\n",
            "Attacks enabled: ['opus16', 'lp8k']\n",
            "\n",
            "[opus16] GOAL=19,903 | ALREADY_DONE(on-disk)=19,903 | TO_DO=0\n",
            "[opus16] Nothing to do.\n",
            "\n",
            "[lp8k] GOAL=19,903 | ALREADY_DONE(on-disk)=13,158 | TO_DO=6,745\n",
            "[lp8k] Backfilling metadata for 158 existing files...\n",
            "[lp8k] Backfill appended.\n",
            "[lp8k] Will process 6,745 new files…\n",
            "[lp8k] 1,000/6,745  (+1000 ok, 0 err)  | elapsed 146.9s\n",
            "[lp8k] 2,000/6,745  (+2000 ok, 0 err)  | elapsed 293.6s\n",
            "[lp8k] 3,000/6,745  (+3000 ok, 0 err)  | elapsed 440.5s\n",
            "[lp8k] 4,000/6,745  (+4000 ok, 0 err)  | elapsed 587.1s\n",
            "[lp8k] 5,000/6,745  (+5000 ok, 0 err)  | elapsed 734.2s\n",
            "[lp8k] 6,000/6,745  (+6000 ok, 0 err)  | elapsed 882.8s\n",
            "[lp8k] 6,745/6,745  (+6745 ok, 0 err)  | elapsed 996.6s\n",
            "[lp8k] finished: +6745 new, 0 errors | total elapsed 996.6s\n",
            "[sum] opus16 → files:   25190 | hours:   56.21 h\n",
            "[sum] lp8k   → files:   19903 | hours:   43.54 h\n",
            "[sum] TOTAL  → files:   45093 | hours:   99.75 h\n"
          ]
        }
      ],
      "source": [
        "# ===== Codec/Channel Attacks — subset + resumable + no-dup + progress =====\n",
        "import os, time, csv, hashlib, subprocess, tempfile\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np, soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/hindi_dfake\"\n",
        "META = Path(ROOT) / \"metadata\"\n",
        "OUT_DIR = Path(ROOT) / \"processed\" / \"attacks\"\n",
        "ATTACK_CSV = META / \"attacks.csv\"\n",
        "\n",
        "# --- attacks (you can add more later) ---\n",
        "ATTACKS = {\n",
        "    \"opus16\": {\"codec\": \"libopus\",           \"bitrate\": \"16k\",  \"container\": \"ogg\"},\n",
        "    \"lp8k\":   {\"filter\": \"lowpass=f=8000\"},\n",
        "    # \"opus24\": {\"codec\": \"libopus\",           \"bitrate\": \"24k\",  \"container\": \"ogg\"},\n",
        "    # \"opus32\": {\"codec\": \"libopus\",           \"bitrate\": \"32k\",  \"container\": \"ogg\"},\n",
        "    # \"amrnb\":  {\"codec\": \"libopencore_amrnb\", \"bitrate\": \"12.2k\",\"ar\": 8000, \"container\": \"amr\"},\n",
        "}\n",
        "ENABLED_ATTACKS = list(ATTACKS.keys())\n",
        "\n",
        "# --- subset knobs (40% like you asked) ---\n",
        "FRACTION_REAL = 0.40\n",
        "FRACTION_FAKE = 0.40\n",
        "HASH_SEED     = 2025    # stable selection across runs (no randomness drift)\n",
        "\n",
        "MAX_WORKERS = 6\n",
        "BATCH_WRITE = 1000      # write metadata every N successes\n",
        "BACKFILL_METADATA = True  # if output file exists but not in CSV, add a row for it\n",
        "\n",
        "assert 'df_all' in globals() and len(df_all) > 0, \"Run the selection DEBUG cell first; it defines df_all.\"\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def ffmpeg_supports(codec_name: str) -> bool:\n",
        "    try:\n",
        "        out = subprocess.run([\"ffmpeg\",\"-hide_banner\",\"-codecs\"],\n",
        "                             stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                             text=True, check=True).stdout\n",
        "        return codec_name in out\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def run_ffmpeg(cmd_list):\n",
        "    p = subprocess.run(cmd_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    if p.returncode != 0:\n",
        "        raise RuntimeError(p.stderr.decode(\"utf-8\",\"ignore\"))\n",
        "\n",
        "def out_path_for(in_path: str, attack_key: str) -> Path:\n",
        "    return OUT_DIR / attack_key / f\"{Path(in_path).stem}.{attack_key}.wav\"\n",
        "\n",
        "def ensure_parent(p: Path): p.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def wav_duration(p: Path) -> float:\n",
        "    try:\n",
        "        d, sr = sf.read(str(p), dtype=\"float32\", always_2d=False)\n",
        "        return len(d)/float(sr)\n",
        "    except: return 0.0\n",
        "\n",
        "def opus_roundtrip(in_wav: str, out_wav: str, bitrate: str):\n",
        "    with tempfile.TemporaryDirectory() as td:\n",
        "        ogg = str(Path(td)/\"tmp.ogg\")\n",
        "        run_ffmpeg([\"ffmpeg\",\"-loglevel\",\"error\",\"-y\",\"-i\",in_wav,\n",
        "                    \"-ac\",\"1\",\"-ar\",\"16000\",\"-c:a\",\"libopus\",\"-b:a\",bitrate, ogg])\n",
        "        run_ffmpeg([\"ffmpeg\",\"-loglevel\",\"error\",\"-y\",\"-i\",ogg,\n",
        "                    \"-ac\",\"1\",\"-ar\",\"16000\",\"-sample_fmt\",\"s16\", out_wav])\n",
        "\n",
        "def amrnb_roundtrip(in_wav: str, out_wav: str, bitrate: str, ar=8000):\n",
        "    with tempfile.TemporaryDirectory() as td:\n",
        "        amr = str(Path(td)/\"tmp.amr\")\n",
        "        run_ffmpeg([\"ffmpeg\",\"-loglevel\",\"error\",\"-y\",\"-i\",in_wav,\n",
        "                    \"-ac\",\"1\",\"-ar\",str(ar),\"-c:a\",\"libopencore_amrnb\",\"-b:a\",bitrate, amr])\n",
        "        run_ffmpeg([\"ffmpeg\",\"-loglevel\",\"error\",\"-y\",\"-i\",amr,\n",
        "                    \"-ac\",\"1\",\"-ar\",\"16000\",\"-sample_fmt\",\"s16\", out_wav])\n",
        "\n",
        "def filter_only(in_wav: str, out_wav: str, af: str):\n",
        "    run_ffmpeg([\"ffmpeg\",\"-loglevel\",\"error\",\"-y\",\"-i\",in_wav,\n",
        "                \"-ac\",\"1\",\"-ar\",\"16000\",\"-af\",af,\"-sample_fmt\",\"s16\", out_wav])\n",
        "\n",
        "def stable_frac_mask(series: pd.Series, frac: float, seed: int) -> pd.Series:\n",
        "    \"\"\"Deterministic subset by hashing path; same result every run.\"\"\"\n",
        "    if frac >= 1.0: return pd.Series([True]*len(series), index=series.index)\n",
        "    # make 0..1 score from sha1(path + seed)\n",
        "    s = series.astype(str).apply(lambda p: int(hashlib.sha1((p+str(seed)).encode()).hexdigest()[:8], 16) / 0xFFFFFFFF)\n",
        "    return s < frac\n",
        "\n",
        "# ---------- build subset (stratified by label & source, deterministic) ----------\n",
        "def stratified_subset_hash(df):\n",
        "    parts = []\n",
        "    for (lab, src), g in df.groupby([\"label\",\"source\"], dropna=False):\n",
        "        frac = FRACTION_REAL if lab == \"real\" else FRACTION_FAKE\n",
        "        if frac >= 1.0 or len(g) <= 1:\n",
        "            parts.append(g)\n",
        "        else:\n",
        "            mask = stable_frac_mask(g[\"path\"], frac, HASH_SEED)\n",
        "            parts.append(g[mask])\n",
        "    return pd.concat(parts, ignore_index=True) if parts else df\n",
        "\n",
        "df_input = stratified_subset_hash(df_all.copy())\n",
        "print(\"Subset sizes by label/source:\")\n",
        "print(df_input.groupby([\"label\",\"source\"]).size().to_string())\n",
        "\n",
        "# ---------- skip work we already did ----------\n",
        "# paths already listed in attacks.csv\n",
        "existing_paths_csv = set()\n",
        "if ATTACK_CSV.exists():\n",
        "    try:\n",
        "        existing_paths_csv = set(pd.read_csv(ATTACK_CSV, usecols=[\"path\"])[\"path\"].astype(str))\n",
        "    except Exception:\n",
        "        existing_paths_csv = set()\n",
        "\n",
        "# filter attacks by codec availability\n",
        "enabled = []\n",
        "for k in ENABLED_ATTACKS:\n",
        "    spec = ATTACKS[k]\n",
        "    if \"codec\" in spec and not ffmpeg_supports(spec[\"codec\"]):\n",
        "        print(f\"[WARN] Disabling {k}: ffmpeg missing {spec['codec']}\")\n",
        "        continue\n",
        "    enabled.append(k)\n",
        "ENABLED_ATTACKS = enabled\n",
        "print(\"Attacks enabled:\", ENABLED_ATTACKS)\n",
        "\n",
        "# ensure metadata exists with header for incremental appends\n",
        "ATTACK_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
        "if not ATTACK_CSV.exists():\n",
        "    with open(ATTACK_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"parent_utt\",\"path\",\"duration\",\"speaker_id\",\"gender\",\"text\",\"label\",\n",
        "                    \"fake_type\",\"attack\",\"codec\",\"bitrate\",\"source\"])\n",
        "\n",
        "def append_rows(rows):\n",
        "    if not rows: return\n",
        "    with open(ATTACK_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=[\"parent_utt\",\"path\",\"duration\",\"speaker_id\",\"gender\",\"text\",\"label\",\n",
        "                                          \"fake_type\",\"attack\",\"codec\",\"bitrate\",\"source\"])\n",
        "        for r in rows:\n",
        "            w.writerow(r)\n",
        "\n",
        "# ---------- run per attack with explicit GOAL/DONE/REMAINING ----------\n",
        "for atk in ENABLED_ATTACKS:\n",
        "    spec = ATTACKS[atk]\n",
        "\n",
        "    # planned outputs for our subset\n",
        "    planned = df_input[\"path\"].astype(str).apply(lambda p: str(out_path_for(p, atk)))\n",
        "    planned_set = set(planned)\n",
        "\n",
        "    # what's already on disk (for this attack)\n",
        "    out_dir = OUT_DIR / atk\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    on_disk = set(str(p) for p in out_dir.glob(\"*.wav\"))\n",
        "\n",
        "    # already done = on disk (we treat as done even if CSV missed a row; we'll backfill)\n",
        "    already_done = planned_set & on_disk\n",
        "    # remaining = planned - on_disk\n",
        "    remaining_set = planned_set - on_disk\n",
        "\n",
        "    goal = len(planned_set)\n",
        "    done = len(already_done)\n",
        "    todo = len(remaining_set)\n",
        "    print(f\"\\n[{atk}] GOAL={goal:,} | ALREADY_DONE(on-disk)={done:,} | TO_DO={todo:,}\")\n",
        "\n",
        "    # (optional) backfill CSV rows for outputs that exist but are missing from CSV\n",
        "    if BACKFILL_METADATA:\n",
        "        missing_meta = [p for p in already_done if p not in existing_paths_csv]\n",
        "        if missing_meta:\n",
        "            print(f\"[{atk}] Backfilling metadata for {len(missing_meta):,} existing files...\")\n",
        "            rows = []\n",
        "            # build a map from input path -> parent row for quick lookup\n",
        "            in2row = {str(r[\"path\"]): r for _, r in df_input.iterrows()}\n",
        "            for out_p in missing_meta:\n",
        "                # reconstruct parent input path from filename stem (before .atk.wav) by searching\n",
        "                stem = Path(out_p).name.split(f\".{atk}.wav\")[0]\n",
        "                # find any input whose stem matches (fast check)\n",
        "                # this is approximate but works because we preserve input stem in output name\n",
        "                cand = [r for r in in2row.values() if Path(str(r[\"path\"])).stem == stem]\n",
        "                base = cand[0] if cand else None\n",
        "                dur = wav_duration(Path(out_p))\n",
        "                rows.append({\n",
        "                    \"parent_utt\": \"\" if base is None else base.get(\"utt_id\",\"\"),\n",
        "                    \"path\": out_p,\n",
        "                    \"duration\": round(dur,3),\n",
        "                    \"speaker_id\": \"\" if base is None else base.get(\"speaker_id\",\"\"),\n",
        "                    \"gender\": \"\" if base is None else base.get(\"gender\",\"\"),\n",
        "                    \"text\": \"\" if base is None else base.get(\"text\",\"\"),\n",
        "                    \"label\": \"\" if base is None else base.get(\"label\",\"\"),\n",
        "                    \"fake_type\": \"channel_attack\",\n",
        "                    \"attack\": atk,\n",
        "                    \"codec\": spec.get(\"codec\", spec.get(\"filter\",\"\")),\n",
        "                    \"bitrate\": spec.get(\"bitrate\",\"\"),\n",
        "                    \"source\": \"\" if base is None else base.get(\"source\",\"\"),\n",
        "                })\n",
        "            append_rows(rows)\n",
        "            existing_paths_csv.update(missing_meta)\n",
        "            print(f\"[{atk}] Backfill appended.\")\n",
        "\n",
        "    if todo == 0:\n",
        "        print(f\"[{atk}] Nothing to do.\")\n",
        "        continue\n",
        "\n",
        "    # build rows to process (Series aligned with df_input)\n",
        "    to_do_mask = planned.isin(list(remaining_set))\n",
        "    df_todo = df_input[to_do_mask].copy()\n",
        "    n = len(df_todo)\n",
        "    print(f\"[{atk}] Will process {n:,} new files…\")\n",
        "\n",
        "    created = 0; errs = 0\n",
        "    buffer_rows = []\n",
        "\n",
        "    def apply_one(row):\n",
        "        in_wav = str(row[\"path\"])\n",
        "        out_wav = str(out_path_for(in_wav, atk))\n",
        "        outp = Path(out_wav)\n",
        "        ensure_parent(outp)\n",
        "        try:\n",
        "            if spec.get(\"codec\") == \"libopus\":\n",
        "                opus_roundtrip(in_wav, out_wav, spec[\"bitrate\"])\n",
        "            elif spec.get(\"codec\") == \"libopencore_amrnb\":\n",
        "                amrnb_roundtrip(in_wav, out_wav, spec[\"bitrate\"], spec.get(\"ar\", 8000))\n",
        "            elif \"filter\" in spec:\n",
        "                filter_only(in_wav, out_wav, spec[\"filter\"])\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown attack spec: {spec}\")\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                if outp.exists(): outp.unlink()\n",
        "            except: pass\n",
        "            return {\"error\": str(e), \"path\": in_wav}\n",
        "\n",
        "        dur = wav_duration(outp)\n",
        "        if dur < 0.2:\n",
        "            try: outp.unlink()\n",
        "            except: pass\n",
        "            return {\"error\": \"too_short\", \"path\": in_wav}\n",
        "\n",
        "        return {\n",
        "            \"parent_utt\": row.get(\"utt_id\",\"\"),\n",
        "            \"path\": out_wav,\n",
        "            \"duration\": round(dur, 3),\n",
        "            \"speaker_id\": row.get(\"speaker_id\",\"\"),\n",
        "            \"gender\": row.get(\"gender\",\"\"),\n",
        "            \"text\": row.get(\"text\",\"\"),\n",
        "            \"label\": row.get(\"label\",\"\"),\n",
        "            \"fake_type\": \"channel_attack\",\n",
        "            \"attack\": atk,\n",
        "            \"codec\": spec.get(\"codec\", spec.get(\"filter\",\"\")),\n",
        "            \"bitrate\": spec.get(\"bitrate\",\"\"),\n",
        "            \"source\": row.get(\"source\",\"\"),\n",
        "        }\n",
        "\n",
        "    t0 = time.time()\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "        futs = [ex.submit(apply_one, r) for _, r in df_todo.iterrows()]\n",
        "        done_ct = 0\n",
        "        for f in as_completed(futs):\n",
        "            done_ct += 1\n",
        "            res = f.result()\n",
        "            if isinstance(res, dict) and \"error\" in res:\n",
        "                errs += 1\n",
        "            else:\n",
        "                buffer_rows.append(res)\n",
        "                created += 1\n",
        "\n",
        "            # periodic progress & metadata flush\n",
        "            if done_ct % 1000 == 0 or (done_ct == n):\n",
        "                print(f\"[{atk}] {done_ct:,}/{n:,}  (+{created} ok, {errs} err)  | elapsed {time.time()-t0:.1f}s\", flush=True)\n",
        "\n",
        "            if len(buffer_rows) >= BATCH_WRITE or (done_ct == n and buffer_rows):\n",
        "                append_rows(buffer_rows)\n",
        "                existing_paths_csv.update([r[\"path\"] for r in buffer_rows])\n",
        "                buffer_rows.clear()\n",
        "\n",
        "    print(f\"[{atk}] finished: +{created} new, {errs} errors | total elapsed {time.time()-t0:.1f}s\")\n",
        "\n",
        "# quick per-attack summary (files/hours on disk)\n",
        "def folder_hours(d: Path):\n",
        "    s = 0.0\n",
        "    for p in d.glob(\"*.wav\"):\n",
        "        try:\n",
        "            x, sr = sf.read(str(p)); s += len(x)/float(sr)\n",
        "        except: pass\n",
        "    return s/3600\n",
        "\n",
        "tot_files = 0; tot_hours = 0.0\n",
        "for atk in ENABLED_ATTACKS:\n",
        "    d = OUT_DIR / atk\n",
        "    if not d.exists(): continue\n",
        "    n = len(list(d.glob(\"*.wav\")))\n",
        "    h = folder_hours(d)\n",
        "    tot_files += n; tot_hours += h\n",
        "    print(f\"[sum] {atk:6s} → files: {n:7d} | hours: {h:7.2f} h\")\n",
        "print(f\"[sum] TOTAL  → files: {tot_files:7d} | hours: {tot_hours:7.2f} h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Source Labeling Helper**\n",
        "We define a helper function to standardize source labels (e.g., `real_commonvoice`, `fake_tts`) based on file paths. This ensures consistency across all metadata files."
      ],
      "metadata": {
        "id": "NQxUQcic512Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biDgk540YOht"
      },
      "outputs": [],
      "source": [
        "def src_from_path(p: str) -> str:\n",
        "    p = str(p)\n",
        "    if \"/raw/real_clean/commonvoice\" in p: return \"real_commonvoice\"\n",
        "    if \"/raw/real_clean/indictts\"   in p: return \"real_indictts\"\n",
        "    if \"/raw/real_clean/ivr\"        in p: return \"real_ivr\"\n",
        "    if \"/raw/fake_tts\"              in p: return \"fake_tts\"\n",
        "    if \"/raw/fake_vc\"               in p: return \"fake_vc\"     # <— add this later\n",
        "    return \"other\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDPJOkrTceU8"
      },
      "source": [
        "### **4. Attack Metadata Cleaning (Robust)**\n",
        "We perform a deep clean of the attack metadata by cross-referencing the `master_real.csv` and `master_fake.csv` files. This ensures that every attacked file (even those recovered via backfill) inherits the correct label, speaker ID, and text from its original parent file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgmviWjnZFI3",
        "outputId": "18f84976-0e24-479a-d032-e41867aa1209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /content/drive/MyDrive/hindi_dfake/metadata/attacks.labeled.csv | rows=45093\n",
            "\n",
            "Label counts:\n",
            "label\n",
            "real    23658\n",
            "fake    21435\n",
            "\n",
            "By attack × label:\n",
            "attack label  files     hours\n",
            "  lp8k  fake   9567 22.377793\n",
            "  lp8k  real  10336 21.163750\n",
            "opus16  fake  11868 27.002213\n",
            "opus16  real  13322 29.203304\n"
          ]
        }
      ],
      "source": [
        "# === Clean + label attacks.clean.csv using master manifests (robust) ===\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/hindi_dfake\"\n",
        "META = Path(ROOT)/\"metadata\"\n",
        "\n",
        "ATT_CLEAN = META/\"attacks.clean.csv\"\n",
        "REAL_CSV  = META/\"master_real.csv\"\n",
        "FAKE_CSV  = META/\"master_fake.csv\"\n",
        "\n",
        "assert ATT_CLEAN.exists(), f\"Missing: {ATT_CLEAN}\"\n",
        "assert REAL_CSV.exists() and FAKE_CSV.exists(), \"Missing master_real.csv or master_fake.csv\"\n",
        "\n",
        "# ---- helpers ----\n",
        "def src_from_path(p: str) -> str:\n",
        "    p = str(p)\n",
        "    if \"/raw/real_clean/commonvoice\" in p: return \"real_commonvoice\"\n",
        "    if \"/raw/real_clean/indictts\"   in p: return \"real_indictts\"\n",
        "    if \"/raw/real_clean/ivr\"        in p: return \"real_ivr\"\n",
        "    if \"/raw/fake_tts\"              in p: return \"fake_tts\"\n",
        "    if \"/raw/fake_vc\"               in p: return \"fake_vc\"\n",
        "    return \"other\"\n",
        "\n",
        "from pathlib import Path as _P\n",
        "def orig_stem_from_attacked(path: str, attack: str) -> str:\n",
        "    s = _P(path).stem\n",
        "    suf = f\".{attack}\"\n",
        "    return s[:-len(suf)] if s.endswith(suf) else s\n",
        "\n",
        "# ---- load and normalize attacks.clean.csv ----\n",
        "dfA = pd.read_csv(ATT_CLEAN)\n",
        "# ensure expected cols exist; keep current values in *_cur to avoid suffix mess after merge\n",
        "for c in [\"parent_utt\",\"speaker_id\",\"gender\",\"text\",\"label\",\"source\",\"fake_type\",\"duration\",\"attack\",\"codec\",\"bitrate\",\"path\"]:\n",
        "    if c not in dfA.columns:\n",
        "        dfA[c] = \"\" if c != \"duration\" else 0.0\n",
        "rename_map = {\n",
        "    \"parent_utt\":\"parent_utt_cur\",\n",
        "    \"speaker_id\":\"speaker_id_cur\",\n",
        "    \"gender\":\"gender_cur\",\n",
        "    \"text\":\"text_cur\",\n",
        "    \"label\":\"label_cur\",\n",
        "    \"source\":\"source_cur\",\n",
        "}\n",
        "dfA = dfA.rename(columns=rename_map)\n",
        "dfA[\"duration\"] = pd.to_numeric(dfA[\"duration\"], errors=\"coerce\").fillna(0.0)\n",
        "dfA[\"orig_stem\"] = [orig_stem_from_attacked(p, a) for p, a in zip(dfA[\"path\"], dfA[\"attack\"])]\n",
        "\n",
        "# ---- build stem->meta map from masters ----\n",
        "def build_map(df_in, parent_label):\n",
        "    rows = []\n",
        "    for _, r in df_in.iterrows():\n",
        "        rp = str(r.get(\"path\",\"\"))\n",
        "        rows.append({\n",
        "            \"stem\": _P(rp).stem,\n",
        "            \"m_label\": parent_label,\n",
        "            \"m_source\": src_from_path(rp),\n",
        "            \"m_fake_type\": str(r.get(\"fake_type\",\"\")) if parent_label==\"fake\" else \"\",\n",
        "            \"m_utt\": str(r.get(\"utt_id\",\"\")),\n",
        "            \"m_speaker\": str(r.get(\"speaker_id\",\"\")),\n",
        "            \"m_gender\": str(r.get(\"gender\",\"\")),\n",
        "            \"m_text\": str(r.get(\"text\",\"\")),\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "dfR = pd.read_csv(REAL_CSV)\n",
        "dfF = pd.read_csv(FAKE_CSV)\n",
        "\n",
        "dfMap = pd.concat([build_map(dfR, \"real\"), build_map(dfF, \"fake\")], ignore_index=True)\n",
        "dfMap = dfMap.drop_duplicates(subset=[\"stem\"], keep=\"first\")\n",
        "\n",
        "# ---- merge attacks with map by original stem ----\n",
        "dfM = dfA.merge(dfMap, how=\"left\", left_on=\"orig_stem\", right_on=\"stem\")\n",
        "\n",
        "# ---- fix label -> only {'real','fake'} ----\n",
        "lab_cur = dfM[\"label_cur\"].astype(str).str.lower()\n",
        "lab_map = dfM[\"m_label\"].astype(str).str.lower()\n",
        "ft_map  = dfM[\"m_fake_type\"].astype(str)\n",
        "\n",
        "dfM[\"label\"] = np.where(\n",
        "    lab_cur.isin({\"real\",\"fake\"}), lab_cur,\n",
        "    np.where(lab_map.isin({\"real\",\"fake\"}), lab_map,\n",
        "             np.where(ft_map != \"\", \"fake\", \"\"))\n",
        ")\n",
        "\n",
        "# ---- fix source (prefer current, then map, then infer from path) ----\n",
        "src_cur = dfM[\"source_cur\"].astype(str)\n",
        "src_final = src_cur.where(src_cur != \"\", dfM[\"m_source\"])\n",
        "src_final = src_final.where(src_final != \"\", dfM[\"path\"].astype(str).map(src_from_path))\n",
        "dfM[\"source\"] = src_final\n",
        "\n",
        "# ---- force fake_type for these rows ----\n",
        "dfM[\"fake_type\"] = \"channel_attack\"\n",
        "\n",
        "# ---- backfill parent_utt/speaker/gender/text ----\n",
        "def coalesce(cur, mapped):\n",
        "    cur = cur.astype(str)\n",
        "    mapped = mapped.astype(str)\n",
        "    return np.where(cur != \"\", cur, mapped)\n",
        "\n",
        "dfM[\"parent_utt\"] = coalesce(dfM.get(\"parent_utt_cur\", pd.Series([\"\"]*len(dfM))),\n",
        "                             dfM.get(\"m_utt\", pd.Series([\"\"]*len(dfM))))\n",
        "dfM[\"speaker_id\"] = coalesce(dfM.get(\"speaker_id_cur\", pd.Series([\"\"]*len(dfM))),\n",
        "                             dfM.get(\"m_speaker\", pd.Series([\"\"]*len(dfM))))\n",
        "dfM[\"gender\"]     = coalesce(dfM.get(\"gender_cur\", pd.Series([\"\"]*len(dfM))),\n",
        "                             dfM.get(\"m_gender\", pd.Series([\"\"]*len(dfM))))\n",
        "dfM[\"text\"]       = coalesce(dfM.get(\"text_cur\", pd.Series([\"\"]*len(dfM))),\n",
        "                             dfM.get(\"m_text\", pd.Series([\"\"]*len(dfM))))\n",
        "\n",
        "# ---- final tidy dataframe ----\n",
        "out_cols = [\"parent_utt\",\"path\",\"duration\",\"speaker_id\",\"gender\",\"text\",\"label\",\n",
        "            \"fake_type\",\"attack\",\"codec\",\"bitrate\",\"source\"]\n",
        "df_out = dfM[out_cols].copy()\n",
        "\n",
        "OUT = META/\"attacks.labeled.csv\"\n",
        "df_out.to_csv(OUT, index=False)\n",
        "print(f\"Saved: {OUT} | rows={len(df_out)}\")\n",
        "\n",
        "# ---- quick summaries ----\n",
        "def hours_series(s: pd.Series) -> float:\n",
        "    return float(pd.to_numeric(s, errors=\"coerce\").fillna(0.0).sum()/3600.0)\n",
        "\n",
        "print(\"\\nLabel counts:\")\n",
        "print(df_out[\"label\"].value_counts(dropna=False).to_string())\n",
        "\n",
        "print(\"\\nBy attack × label:\")\n",
        "print(\n",
        "    df_out.groupby([\"attack\",\"label\"], dropna=False)\n",
        "          .agg(files=(\"path\",\"count\"), hours=(\"duration\", hours_series))\n",
        "          .reset_index()\n",
        "          .sort_values([\"attack\",\"label\"])\n",
        "          .to_string(index=False)\n",
        ")"
      ]
    }
  ]
}