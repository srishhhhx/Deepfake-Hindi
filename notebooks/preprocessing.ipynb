{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 1: Environment Setup & Dependencies**\n",
        "In this section, we configure the runtime environment, mount persistent storage (Google Drive), and authenticate with Hugging Face to access the required datasets."
      ],
      "metadata": {
        "id": "jYLI07ijHUQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Project Setup & Directory Initialization**\n",
        "We initialize the project structure by creating the necessary subdirectories for raw data, processed features, metadata, and model checkpoints."
      ],
      "metadata": {
        "id": "v8rVl7jrFzq1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dCNl237M_Kr"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/hindi_dfake/{raw,processed,metadata,scripts,checkpoints}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Environment Configuration**\n",
        "We mount Google Drive to ensure persistent storage and dynamically define the root paths (`ROOT_DIR`, `OUT_DIR`, etc.) to avoid hardcoding errors across different sessions."
      ],
      "metadata": {
        "id": "Zft52MN2Gr2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4eXmEHUZ1DP",
        "outputId": "4232c0b9-70d3-4191-eddf-37df6a0a6b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive is already mounted here → /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, glob\n",
        "MOUNT_PATH = \"/content/drive\" if os.path.isdir(\"/content/drive/MyDrive\") else \"/content/gdrive\"\n",
        "print(\"Drive is already mounted here →\", MOUNT_PATH)\n",
        "ROOT_DIR = f\"{MOUNT_PATH}/MyDrive/hindi_dfake\"\n",
        "OUT_DIR  = f\"{ROOT_DIR}/raw/real_clean/ivr\"\n",
        "CSV_PATH = f\"{ROOT_DIR}/metadata/master_real.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Hugging Face Authentication**\n",
        "Authentication is required to download the **IndicVoices-R** and **Common Voice** datasets directly from the Hugging Face Hub."
      ],
      "metadata": {
        "id": "zjux7tS5Gu4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "3d162393b84e41b8a1c6cc279e292827",
            "a8a36cdbf3704e29ad7c012fa5eff920",
            "0e55fd890f8f467a952ecac007bd64e8",
            "edfe3d2b8b2f49e6b7d127111c86b58b",
            "a0d2d38dfb0c4d43a3aec03152ded0d1",
            "20a24eecb7894fe4a835117815d4a804",
            "181e2d098b424310a55517eeb47737f0",
            "a7b814943b454759b4f207fa95011a1e",
            "377be24bb3504334adb534383019c715",
            "44b365d69b244d75bd13f24478a5b3c8",
            "0bf243b8172045c4ac92407d1b4c7c4e",
            "b49e60e1f4f544af853d4a889d75ac80",
            "db0eee444be14017a800f361d8642c27",
            "68e8dad5df2645869c8dd6675845d9bd",
            "ce058c87323b4a1887005d4d8bca6dbb",
            "176b79b7fb60414cb0dcdbfc068c612b",
            "479258ee28ad4e88a0829e1697681250",
            "b11cb96865cf4f659853ed4a7216591c",
            "35500233ad49478c8a7a38e2c1631911",
            "f5bf6da094f947ca9054667917cb3829"
          ]
        },
        "id": "0fQKZ7FJS69O",
        "outputId": "8ab170bf-7428-4316-c982-e7d273487ccb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d162393b84e41b8a1c6cc279e292827"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BQHOf6EghIa"
      },
      "source": [
        "# **Section 2: Preprocessing**\n",
        "We preprocess the dataset into `base` (normalized), `strong` (augmented with RawBoost/EQ), and `clean` (EQ-only) profiles to ensure robust training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Metadata Audit**\n",
        "We scan all available CSV files to verify that our `real` and `fake` labels are consistent before merging them into a final training set."
      ],
      "metadata": {
        "id": "Ac4vyXSMG6sx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9h5RCrVh5Na",
        "outputId": "308638b2-cb84-40d1-8e57-da9680f99ebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/hindi_dfake/metadata/attacks.clean.csv \n",
            " label\n",
            "real        20672\n",
            "fake        17334\n",
            "NaN          5287\n",
            "tts_edge     1800\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "/content/drive/MyDrive/hindi_dfake/metadata/attacks.csv \n",
            " label\n",
            "real        20672\n",
            "fake        17334\n",
            "tts_edge     1800\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "/content/drive/MyDrive/hindi_dfake/metadata/attacks.labeled.csv \n",
            " label\n",
            "real    23658\n",
            "fake    21435\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "/content/drive/MyDrive/hindi_dfake/metadata/master_fake.csv \n",
            " label\n",
            "fake    23731\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "/content/drive/MyDrive/hindi_dfake/metadata/master_real.csv \n",
            " label\n",
            "real    25965\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "/content/drive/MyDrive/hindi_dfake/metadata/proc_manifest.csv (no 'label' column) {os.path.basename(f)} — skipped\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, glob, os\n",
        "root = \"/content/drive/MyDrive/hindi_dfake/metadata\"\n",
        "for f in sorted(glob.glob(f\"{root}/*.csv\")):\n",
        "    try:\n",
        "        df = pd.read_csv(f, nrows=5)  # peek header fast\n",
        "        if \"label\" in df.columns:\n",
        "            df = pd.read_csv(f, usecols=[\"label\"])\n",
        "            print(f, \"\\n\", df[\"label\"].value_counts(dropna=False), \"\\n\")\n",
        "        else:\n",
        "            print(f, \"(no 'label' column) {os.path.basename(f)} — skipped\\n\")\n",
        "    except Exception as e:\n",
        "        print(f, \"(read issue) {os.path.basename(f)} — {e}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj4TBH2ikFRc"
      },
      "source": [
        "### **2. Pre-Processing Probe**\n",
        "We test the FFmpeg filter chain (Silence Removal + 16kHz Resampling) on a single file to ensure validity before bulk processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gus8Uq9Egnd2",
        "outputId": "42379a99-49e6-4c5b-84c6-049dadb2eedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample file: /content/drive/MyDrive/hindi_dfake/raw/real_clean/ivr/1a0b61275bc58baf.wav\n",
            "Filter: highpass=f=20,volume=0.0dB,silenceremove=start_periods=1:start_duration=0.2:start_threshold=-45dB:stop_periods=1:stop_duration=0.2:stop_threshold=-45dB\n",
            "Return code: 0\n",
            "OK. Out shape: (129127,) sr: 16000 dur(s): 8.0704375\n"
          ]
        }
      ],
      "source": [
        "# --- PROBE ONE FILE to see ffmpeg/filter errors clearly ---\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import subprocess, tempfile, soundfile as sf, numpy as np\n",
        "\n",
        "ROOT = Path(\"/content/drive/MyDrive/hindi_dfake\")\n",
        "META = ROOT/\"metadata\"\n",
        "REAL = pd.read_csv(META/\"master_real.csv\")\n",
        "FAKE = pd.read_csv(META/\"master_fake.csv\")\n",
        "df = pd.concat([REAL, FAKE], ignore_index=True)\n",
        "df = df[df[\"path\"].apply(lambda p: Path(p).exists())]\n",
        "sample = df.iloc[0][\"path\"]\n",
        "print(\"Sample file:\", sample)\n",
        "\n",
        "TRIM_THR_DB = -45\n",
        "TRIM_DUR_S  = 0.20\n",
        "GAIN_DB     = 0.0  # just to check filter wiring\n",
        "PROFILE     = \"base\"\n",
        "\n",
        "filt_parts = []\n",
        "filt_parts.append(\"highpass=f=20\")\n",
        "# no EQ for base\n",
        "# per-file gain\n",
        "filt_parts.append(f\"volume={GAIN_DB}dB\")\n",
        "# NOTE: use start_duration/stop_duration (not start_silence/stop_silence)\n",
        "filt_parts.append(f\"silenceremove=start_periods=1:start_duration={TRIM_DUR_S}:start_threshold={TRIM_THR_DB}dB:\"\n",
        "                  f\"stop_periods=1:stop_duration={TRIM_DUR_S}:stop_threshold={TRIM_THR_DB}dB\")\n",
        "FILTER = \",\".join(filt_parts)\n",
        "\n",
        "tmp = Path(tempfile.gettempdir())/ \"probe_out.wav\"\n",
        "cmd = [\n",
        "    \"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "    \"-i\", sample,\n",
        "    \"-ac\",\"1\",\"-ar\",\"16000\",\"-af\", FILTER, \"-sample_fmt\",\"s16\",\n",
        "    str(tmp)\n",
        "]\n",
        "print(\"Filter:\", FILTER)\n",
        "p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "print(\"Return code:\", p.returncode)\n",
        "if p.returncode != 0:\n",
        "    print(\"----- STDERR -----\\n\", p.stderr[:1200])\n",
        "else:\n",
        "    try:\n",
        "        import soundfile as sf, numpy as np\n",
        "        x, sr = sf.read(str(tmp), dtype=\"float32\")\n",
        "        print(\"OK. Out shape:\", x.shape, \"sr:\", sr, \"dur(s):\", len(x)/sr)\n",
        "    except Exception as e:\n",
        "        print(\"Wrote file but could not read back:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Profile: Base**\n",
        "We normalize the dataset using the `base` profile (20Hz high-pass, -26dB volume, silence removal) to produce standardized 16kHz mono audio."
      ],
      "metadata": {
        "id": "AOHHaO5wZFNB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwleTbzIutum",
        "outputId": "f88aae33-7c18-4fb2-af14-afe36247fa3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input counts (normalized):\n",
            "label\n",
            "fake    45166\n",
            "real    49623\n",
            "\n",
            "[preproc:base] PLANNED=94,789 | ALREADY_DONE=20,774 | TO_DO=74,015\n",
            "[base] 500/74,015  (+500 ok, 0 err)  | elapsed 116.6s\n",
            "[base] 1,000/74,015  (+1000 ok, 0 err)  | elapsed 158.5s\n",
            "[base] 1,500/74,015  (+1500 ok, 0 err)  | elapsed 222.7s\n",
            "[base] 2,000/74,015  (+2000 ok, 0 err)  | elapsed 290.0s\n",
            "[base] 2,500/74,015  (+2500 ok, 0 err)  | elapsed 358.5s\n",
            "[base] 3,000/74,015  (+3000 ok, 0 err)  | elapsed 425.7s\n",
            "[base] 3,500/74,015  (+3500 ok, 0 err)  | elapsed 489.8s\n",
            "[base] 4,000/74,015  (+4000 ok, 0 err)  | elapsed 555.2s\n",
            "[base] 4,500/74,015  (+4500 ok, 0 err)  | elapsed 618.7s\n",
            "[base] 5,000/74,015  (+5000 ok, 0 err)  | elapsed 682.8s\n",
            "[base] 5,500/74,015  (+5500 ok, 0 err)  | elapsed 746.5s\n",
            "[base] 6,000/74,015  (+6000 ok, 0 err)  | elapsed 811.0s\n",
            "[base] 6,500/74,015  (+6500 ok, 0 err)  | elapsed 875.9s\n",
            "[base] 7,000/74,015  (+7000 ok, 0 err)  | elapsed 939.7s\n",
            "[base] 7,500/74,015  (+7500 ok, 0 err)  | elapsed 1003.9s\n",
            "[base] 8,000/74,015  (+8000 ok, 0 err)  | elapsed 1069.0s\n",
            "[base] 8,500/74,015  (+8500 ok, 0 err)  | elapsed 1132.8s\n",
            "[base] 9,000/74,015  (+9000 ok, 0 err)  | elapsed 1197.2s\n",
            "[base] 9,500/74,015  (+9500 ok, 0 err)  | elapsed 1262.3s\n",
            "[base] 10,000/74,015  (+10000 ok, 0 err)  | elapsed 1327.0s\n",
            "[base] 10,500/74,015  (+10500 ok, 0 err)  | elapsed 1392.5s\n",
            "[base] 11,000/74,015  (+11000 ok, 0 err)  | elapsed 1455.4s\n",
            "[base] 11,500/74,015  (+11500 ok, 0 err)  | elapsed 1522.0s\n",
            "[base] 12,000/74,015  (+12000 ok, 0 err)  | elapsed 1589.6s\n",
            "[base] 12,500/74,015  (+12500 ok, 0 err)  | elapsed 1657.8s\n",
            "[base] 13,000/74,015  (+13000 ok, 0 err)  | elapsed 1721.5s\n",
            "[base] 13,500/74,015  (+13500 ok, 0 err)  | elapsed 1785.0s\n",
            "[base] 14,000/74,015  (+14000 ok, 0 err)  | elapsed 1850.2s\n",
            "[base] 14,500/74,015  (+14500 ok, 0 err)  | elapsed 1913.3s\n",
            "[base] 15,000/74,015  (+15000 ok, 0 err)  | elapsed 1976.5s\n",
            "[base] 15,500/74,015  (+15500 ok, 0 err)  | elapsed 2041.9s\n",
            "[base] 16,000/74,015  (+16000 ok, 0 err)  | elapsed 2105.6s\n",
            "[base] 16,500/74,015  (+16500 ok, 0 err)  | elapsed 2169.3s\n",
            "[base] 17,000/74,015  (+17000 ok, 0 err)  | elapsed 2234.8s\n",
            "[base] 17,500/74,015  (+17500 ok, 0 err)  | elapsed 2298.4s\n",
            "[base] 18,000/74,015  (+18000 ok, 0 err)  | elapsed 2362.9s\n",
            "[base] 18,500/74,015  (+18500 ok, 0 err)  | elapsed 2427.0s\n",
            "[base] 19,000/74,015  (+19000 ok, 0 err)  | elapsed 2491.2s\n",
            "[base] 19,500/74,015  (+19500 ok, 0 err)  | elapsed 2556.4s\n",
            "[base] 20,000/74,015  (+20000 ok, 0 err)  | elapsed 2619.5s\n",
            "[base] 20,500/74,015  (+20500 ok, 0 err)  | elapsed 2683.6s\n",
            "[base] 21,000/74,015  (+21000 ok, 0 err)  | elapsed 2750.9s\n",
            "[base] 21,500/74,015  (+21500 ok, 0 err)  | elapsed 2818.8s\n",
            "[base] 22,000/74,015  (+22000 ok, 0 err)  | elapsed 2886.0s\n",
            "[base] 22,500/74,015  (+22500 ok, 0 err)  | elapsed 2951.2s\n",
            "[base] 23,000/74,015  (+23000 ok, 0 err)  | elapsed 3016.4s\n",
            "[base] 23,500/74,015  (+23500 ok, 0 err)  | elapsed 3080.3s\n",
            "[base] 24,000/74,015  (+24000 ok, 0 err)  | elapsed 3146.0s\n",
            "[base] 24,500/74,015  (+24500 ok, 0 err)  | elapsed 3210.5s\n",
            "[base] 25,000/74,015  (+25000 ok, 0 err)  | elapsed 3275.5s\n",
            "[base] 25,500/74,015  (+25500 ok, 0 err)  | elapsed 3340.8s\n",
            "[base] 26,000/74,015  (+26000 ok, 0 err)  | elapsed 3404.8s\n",
            "[base] 26,500/74,015  (+26500 ok, 0 err)  | elapsed 3470.9s\n",
            "[base] 27,000/74,015  (+27000 ok, 0 err)  | elapsed 3534.8s\n",
            "[base] 27,500/74,015  (+27500 ok, 0 err)  | elapsed 3600.1s\n",
            "[base] 28,000/74,015  (+28000 ok, 0 err)  | elapsed 3665.2s\n",
            "[base] 28,500/74,015  (+28500 ok, 0 err)  | elapsed 3730.0s\n",
            "[base] 29,000/74,015  (+29000 ok, 0 err)  | elapsed 3796.3s\n",
            "[base] 29,500/74,015  (+29500 ok, 0 err)  | elapsed 3861.1s\n",
            "[base] 30,000/74,015  (+30000 ok, 0 err)  | elapsed 3927.6s\n",
            "[base] 30,500/74,015  (+30500 ok, 0 err)  | elapsed 3992.3s\n",
            "[base] 31,000/74,015  (+31000 ok, 0 err)  | elapsed 4059.0s\n",
            "[base] 31,500/74,015  (+31500 ok, 0 err)  | elapsed 4123.1s\n",
            "[base] 32,000/74,015  (+32000 ok, 0 err)  | elapsed 4188.2s\n",
            "[base] 32,500/74,015  (+32500 ok, 0 err)  | elapsed 4253.6s\n",
            "[base] 33,000/74,015  (+33000 ok, 0 err)  | elapsed 4318.7s\n",
            "[base] 33,500/74,015  (+33500 ok, 0 err)  | elapsed 4384.6s\n",
            "[base] 34,000/74,015  (+34000 ok, 0 err)  | elapsed 4448.5s\n",
            "[base] 34,500/74,015  (+34500 ok, 0 err)  | elapsed 4513.5s\n",
            "[base] 35,000/74,015  (+35000 ok, 0 err)  | elapsed 4577.9s\n",
            "[base] 35,500/74,015  (+35500 ok, 0 err)  | elapsed 4641.9s\n",
            "[base] 36,000/74,015  (+36000 ok, 0 err)  | elapsed 4706.9s\n",
            "[base] 36,500/74,015  (+36500 ok, 0 err)  | elapsed 4771.0s\n",
            "[base] 37,000/74,015  (+37000 ok, 0 err)  | elapsed 4835.8s\n",
            "[base] 37,500/74,015  (+37500 ok, 0 err)  | elapsed 4900.9s\n",
            "[base] 38,000/74,015  (+38000 ok, 0 err)  | elapsed 4964.1s\n",
            "[base] 38,500/74,015  (+38500 ok, 0 err)  | elapsed 5029.9s\n",
            "[base] 39,000/74,015  (+39000 ok, 0 err)  | elapsed 5093.9s\n",
            "[base] 39,500/74,015  (+39500 ok, 0 err)  | elapsed 5159.1s\n",
            "[base] 40,000/74,015  (+40000 ok, 0 err)  | elapsed 5226.1s\n",
            "[base] 40,500/74,015  (+40500 ok, 0 err)  | elapsed 5290.1s\n",
            "[base] 41,000/74,015  (+41000 ok, 0 err)  | elapsed 5355.4s\n",
            "[base] 41,500/74,015  (+41500 ok, 0 err)  | elapsed 5419.4s\n",
            "[base] 42,000/74,015  (+42000 ok, 0 err)  | elapsed 5483.3s\n",
            "[base] 42,500/74,015  (+42500 ok, 0 err)  | elapsed 5548.9s\n",
            "[base] 43,000/74,015  (+43000 ok, 0 err)  | elapsed 5612.2s\n",
            "[base] 43,500/74,015  (+43500 ok, 0 err)  | elapsed 5676.7s\n",
            "[base] 44,000/74,015  (+44000 ok, 0 err)  | elapsed 5740.9s\n",
            "[base] 44,500/74,015  (+44500 ok, 0 err)  | elapsed 5804.4s\n",
            "[base] 45,000/74,015  (+45000 ok, 0 err)  | elapsed 5869.7s\n",
            "[base] 45,500/74,015  (+45500 ok, 0 err)  | elapsed 5933.3s\n",
            "[base] 46,000/74,015  (+46000 ok, 0 err)  | elapsed 5996.6s\n",
            "[base] 46,500/74,015  (+46500 ok, 0 err)  | elapsed 6061.8s\n",
            "[base] 47,000/74,015  (+47000 ok, 0 err)  | elapsed 6126.0s\n",
            "[base] 47,500/74,015  (+47500 ok, 0 err)  | elapsed 6190.2s\n",
            "[base] 48,000/74,015  (+48000 ok, 0 err)  | elapsed 6255.0s\n",
            "[base] 48,500/74,015  (+48500 ok, 0 err)  | elapsed 6318.5s\n",
            "[base] 49,000/74,015  (+49000 ok, 0 err)  | elapsed 6380.4s\n",
            "[base] 49,500/74,015  (+49500 ok, 0 err)  | elapsed 6441.9s\n",
            "[base] 50,000/74,015  (+50000 ok, 0 err)  | elapsed 6503.6s\n",
            "[base] 50,500/74,015  (+50500 ok, 0 err)  | elapsed 6567.3s\n",
            "[base] 51,000/74,015  (+51000 ok, 0 err)  | elapsed 6633.8s\n",
            "[base] 51,500/74,015  (+51500 ok, 0 err)  | elapsed 6698.8s\n",
            "[base] 52,000/74,015  (+52000 ok, 0 err)  | elapsed 6762.8s\n",
            "[base] 52,500/74,015  (+52500 ok, 0 err)  | elapsed 6829.1s\n",
            "[base] 53,000/74,015  (+53000 ok, 0 err)  | elapsed 6892.7s\n",
            "[base] 53,500/74,015  (+53500 ok, 0 err)  | elapsed 6956.9s\n",
            "[base] 54,000/74,015  (+54000 ok, 0 err)  | elapsed 7021.3s\n",
            "[base] 54,500/74,015  (+54500 ok, 0 err)  | elapsed 7085.6s\n",
            "[base] 55,000/74,015  (+55000 ok, 0 err)  | elapsed 7149.8s\n",
            "[base] 55,500/74,015  (+55500 ok, 0 err)  | elapsed 7214.5s\n",
            "[base] 56,000/74,015  (+56000 ok, 0 err)  | elapsed 7277.1s\n",
            "[base] 56,500/74,015  (+56500 ok, 0 err)  | elapsed 7340.3s\n",
            "[base] 57,000/74,015  (+57000 ok, 0 err)  | elapsed 7405.6s\n",
            "[base] 57,500/74,015  (+57500 ok, 0 err)  | elapsed 7470.4s\n",
            "[base] 58,000/74,015  (+58000 ok, 0 err)  | elapsed 7536.9s\n",
            "[base] 58,500/74,015  (+58500 ok, 0 err)  | elapsed 7602.6s\n",
            "[base] 59,000/74,015  (+59000 ok, 0 err)  | elapsed 7669.1s\n",
            "[base] 59,500/74,015  (+59500 ok, 0 err)  | elapsed 7734.4s\n",
            "[base] 60,000/74,015  (+60000 ok, 0 err)  | elapsed 7800.7s\n",
            "[base] 60,500/74,015  (+60500 ok, 0 err)  | elapsed 7866.0s\n",
            "[base] 61,000/74,015  (+61000 ok, 0 err)  | elapsed 7932.3s\n",
            "[base] 61,500/74,015  (+61500 ok, 0 err)  | elapsed 7996.3s\n",
            "[base] 62,000/74,015  (+62000 ok, 0 err)  | elapsed 8060.3s\n",
            "[base] 62,500/74,015  (+62500 ok, 0 err)  | elapsed 8125.5s\n",
            "[base] 63,000/74,015  (+63000 ok, 0 err)  | elapsed 8188.5s\n",
            "[base] 63,500/74,015  (+63500 ok, 0 err)  | elapsed 8252.1s\n",
            "[base] 64,000/74,015  (+64000 ok, 0 err)  | elapsed 8317.5s\n",
            "[base] 64,500/74,015  (+64500 ok, 0 err)  | elapsed 8378.5s\n",
            "[base] 65,000/74,015  (+65000 ok, 0 err)  | elapsed 8441.4s\n",
            "[base] 65,500/74,015  (+65500 ok, 0 err)  | elapsed 8507.3s\n",
            "[base] 66,000/74,015  (+66000 ok, 0 err)  | elapsed 8570.8s\n",
            "[base] 66,500/74,015  (+66500 ok, 0 err)  | elapsed 8635.6s\n",
            "[base] 67,000/74,015  (+67000 ok, 0 err)  | elapsed 8701.9s\n",
            "[base] 67,500/74,015  (+67500 ok, 0 err)  | elapsed 8766.0s\n",
            "[base] 68,000/74,015  (+68000 ok, 0 err)  | elapsed 8831.8s\n",
            "[base] 68,500/74,015  (+68500 ok, 0 err)  | elapsed 8896.9s\n",
            "[base] 69,000/74,015  (+69000 ok, 0 err)  | elapsed 8961.4s\n",
            "[base] 69,500/74,015  (+69500 ok, 0 err)  | elapsed 9027.7s\n",
            "[base] 70,000/74,015  (+70000 ok, 0 err)  | elapsed 9092.2s\n",
            "[base] 70,500/74,015  (+70500 ok, 0 err)  | elapsed 9158.3s\n",
            "[base] 71,000/74,015  (+71000 ok, 0 err)  | elapsed 9222.8s\n",
            "[base] 71,500/74,015  (+71500 ok, 0 err)  | elapsed 9289.1s\n",
            "[base] 72,000/74,015  (+72000 ok, 0 err)  | elapsed 9354.2s\n",
            "[base] 72,500/74,015  (+72500 ok, 0 err)  | elapsed 9419.7s\n",
            "[base] 73,000/74,015  (+73000 ok, 0 err)  | elapsed 9485.3s\n",
            "[base] 73,500/74,015  (+73500 ok, 0 err)  | elapsed 9550.0s\n",
            "[base] 74,000/74,015  (+74000 ok, 0 err)  | elapsed 9616.0s\n",
            "[base] 74,015/74,015  (+74015 ok, 0 err)  | elapsed 9617.8s\n",
            "[base] FINISHED: +74015 files, 0 errors | total elapsed 9617.8s\n",
            "[base] Manifest rows: 85,055 | hours: 68.53h\n"
          ]
        }
      ],
      "source": [
        "# ========== PREPROCESSING (full run; resumable; same-FS temp) ==========\n",
        "# Pipeline: DC high-pass -> (optional artifact EQ) -> per-file volume -> light trim\n",
        "# Writes: /content/drive/MyDrive/hindi_dfake/processed/wav/<profile>/...\n",
        "# Logs  : /content/drive/MyDrive/hindi_dfake/metadata/proc_manifest.csv\n",
        "\n",
        "import os, csv, time, uuid, subprocess, hashlib\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np, soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "ROOT = Path(\"/content/drive/MyDrive/hindi_dfake\")\n",
        "META = ROOT / \"metadata\"\n",
        "OUT_ROOT = ROOT / \"processed\" / \"wav\"\n",
        "\n",
        "PROFILE = \"base\"           # \"base\", \"research_eq\", \"research_rb\", \"strong\"\n",
        "INCLUDE_ATTACKS = True     # include attacks.labeled.csv\n",
        "FRACTION_REAL = 1.00       # process EVERYTHING\n",
        "FRACTION_FAKE = 1.00\n",
        "HASH_SEED = 2025\n",
        "\n",
        "TARGET_DB   = -26.0        # target RMS (dBFS-ish)\n",
        "TRIM_THR_DB = -45          # silence threshold for trim\n",
        "TRIM_DUR_S  = 0.20         # trim head/tail if >= this length\n",
        "\n",
        "MAX_WORKERS = 6            # bump if stable\n",
        "MANIFEST = META / \"proc_manifest.csv\"\n",
        "BATCH_WRITE = 1000\n",
        "\n",
        "REAL_CSV  = META / \"master_real.csv\"\n",
        "FAKE_CSV  = META / \"master_fake.csv\"\n",
        "ATT_CSV   = META / \"attacks.labeled.csv\"   # cleaned labels\n",
        "assert REAL_CSV.exists() and FAKE_CSV.exists(), \"Missing master_real.csv/master_fake.csv\"\n",
        "\n",
        "def log(s): print(s, flush=True)\n",
        "\n",
        "def read_csv_safe(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    for c in [\"utt_id\",\"path\",\"duration\",\"label\",\"fake_type\",\"source\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\" if c != \"duration\" else 0.0\n",
        "    return df\n",
        "\n",
        "def stable_frac_mask(series: pd.Series, frac: float, seed: int) -> pd.Series:\n",
        "    if frac >= 1.0: return pd.Series([True]*len(series), index=series.index)\n",
        "    s = series.astype(str).apply(lambda p: int(hashlib.sha1((p+str(seed)).encode()).hexdigest()[:8], 16) / 0xFFFFFFFF)\n",
        "    return s < frac\n",
        "\n",
        "def out_path_for(in_path: str) -> Path:\n",
        "    p = Path(in_path)\n",
        "    try:    rel = p.relative_to(ROOT)\n",
        "    except: rel = Path(\"_external\") / p.name\n",
        "    return OUT_ROOT / PROFILE / rel\n",
        "\n",
        "def ensure_parent(p: Path): p.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def rms_dbfs(p: Path) -> float:\n",
        "    try:\n",
        "        x, sr = sf.read(str(p), dtype=\"float32\", always_2d=False)\n",
        "        if x.ndim > 1: x = x.mean(axis=1)\n",
        "        if len(x) == 0: return -120.0\n",
        "        rms = float(np.sqrt(np.mean(np.square(x))))\n",
        "        if rms <= 1e-9: return -120.0\n",
        "        return 20.0*np.log10(np.clip(rms, 1e-9, 1.0))\n",
        "    except Exception:\n",
        "        return -120.0\n",
        "\n",
        "def build_filter_chain(profile: str, gain_db: float) -> str:\n",
        "    parts = [\"highpass=f=20\"]\n",
        "    if profile in (\"research_eq\",\"strong\"):\n",
        "        parts += [\n",
        "            \"equalizer=f=3000:t=q:w=1.0:g=2.5\",\n",
        "            \"equalizer=f=4800:t=q:w=0.9:g=2.0\",\n",
        "            \"treble=g=1.0:f=6000:t=h:w=0.7\"\n",
        "        ]\n",
        "    parts.append(f\"volume={gain_db}dB\")\n",
        "    # portable args: start_duration/stop_duration\n",
        "    parts.append(\n",
        "        f\"silenceremove=start_periods=1:start_duration={TRIM_DUR_S}:start_threshold={TRIM_THR_DB}dB:\"\n",
        "        f\"stop_periods=1:stop_duration={TRIM_DUR_S}:stop_threshold={TRIM_THR_DB}dB\"\n",
        "    )\n",
        "    return \",\".join(parts)\n",
        "\n",
        "def duration_s(p: Path) -> float:\n",
        "    try:\n",
        "        x, sr = sf.read(str(p), dtype=\"float32\", always_2d=False)\n",
        "        return len(x)/float(sr) if sr else 0.0\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# -------- Load inputs --------\n",
        "dfR = read_csv_safe(REAL_CSV)\n",
        "dfF = read_csv_safe(FAKE_CSV)\n",
        "dfs = [dfR, dfF]\n",
        "if INCLUDE_ATTACKS and ATT_CSV.exists(): dfs.append(read_csv_safe(ATT_CSV))\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# keep only files that exist\n",
        "df_all = df_all[df_all[\"path\"].astype(str).apply(lambda p: Path(p).exists())].copy()\n",
        "\n",
        "# normalize labels -> only {'real','fake'}\n",
        "def normalize_label(lab, fake_type, src, path):\n",
        "    l = str(lab).strip().lower()\n",
        "    if l in {\"real\",\"fake\"}: return l\n",
        "    ft = str(fake_type).strip().lower()\n",
        "    s  = str(src).strip().lower()\n",
        "    p  = str(path)\n",
        "    if \"/raw/real_clean\" in p or s.startswith(\"real_\"): return \"real\"\n",
        "    if ft in {\"tts_edge\",\"channel_attack\",\"vc\"} or \"/raw/fake_\" in p or s.startswith(\"fake_\"): return \"fake\"\n",
        "    return \"fake\"\n",
        "df_all[\"label\"] = [normalize_label(l, ft, s, p) for l, ft, s, p in\n",
        "                   zip(df_all[\"label\"], df_all[\"fake_type\"], df_all[\"source\"], df_all[\"path\"])]\n",
        "\n",
        "print(\"Input counts (normalized):\")\n",
        "print(df_all.groupby(df_all[\"label\"].astype(str).str.lower()).size().to_string())\n",
        "\n",
        "# deterministic per-label subset (here: everything)\n",
        "parts = []\n",
        "for lab, g in df_all.groupby(df_all[\"label\"].astype(str).str.lower()):\n",
        "    frac = FRACTION_REAL if lab == \"real\" else FRACTION_FAKE\n",
        "    parts.append(g if frac>=1.0 else g[stable_frac_mask(g[\"path\"], frac, HASH_SEED)])\n",
        "df_in = pd.concat(parts, ignore_index=True) if parts else df_all.copy()\n",
        "\n",
        "# -------- Manifest (resume) --------\n",
        "MANIFEST.parent.mkdir(parents=True, exist_ok=True)\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST)\n",
        "    done_set = set(mf.loc[mf[\"profile\"]==PROFILE, \"path_out\"].astype(str))\n",
        "else:\n",
        "    with open(MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "    done_set = set()\n",
        "\n",
        "def already_done(path_in: str) -> bool:\n",
        "    outp = out_path_for(path_in)\n",
        "    return (str(outp) in done_set) or outp.exists()\n",
        "\n",
        "# -------- Worker --------\n",
        "def preprocess_one(row):\n",
        "    pin = str(row[\"path\"])\n",
        "    pout = out_path_for(pin)\n",
        "    if str(pout) in done_set or pout.exists():\n",
        "        return None\n",
        "\n",
        "    ensure_parent(pout)\n",
        "\n",
        "    cur_db  = rms_dbfs(Path(pin))\n",
        "    gain_db = float(np.clip(TARGET_DB - cur_db, -20.0, 20.0))\n",
        "    FILTER  = build_filter_chain(PROFILE, gain_db)\n",
        "\n",
        "    # temp in SAME dir to avoid cross-device link\n",
        "    tmp = pout.parent / f\".{pout.stem}.tmp-{uuid.uuid4().hex}.wav\"\n",
        "    try:\n",
        "        cmd = [\n",
        "            \"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "            \"-i\", pin, \"-ac\",\"1\",\"-ar\",\"16000\",\"-af\", FILTER, \"-sample_fmt\",\"s16\",\n",
        "            str(tmp)\n",
        "        ]\n",
        "        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        if p.returncode != 0:\n",
        "            try: tmp.unlink(missing_ok=True)\n",
        "            except: pass\n",
        "            return {\"error\": p.stderr.decode(\"utf-8\",\"ignore\")[:300], \"path_in\": pin}\n",
        "\n",
        "        os.replace(str(tmp), str(pout))  # atomic within same FS\n",
        "\n",
        "        # quick stats\n",
        "        try:\n",
        "            x, sr = sf.read(str(pout), dtype=\"float32\", always_2d=False)\n",
        "            if x.ndim > 1: x = x.mean(axis=1)\n",
        "            dur = len(x)/float(sr) if sr else 0.0\n",
        "            peak = float(np.max(np.abs(x))) if len(x) else 0.0\n",
        "            rms  = float(np.sqrt(np.mean(np.square(x)))) if len(x) else 0.0\n",
        "            rms_db = -120.0 if rms<=1e-9 else 20*np.log10(np.clip(rms, 1e-9, 1.0))\n",
        "            peak_db = -120.0 if peak<=1e-9 else 20*np.log10(np.clip(peak, 1e-9, 1.0))\n",
        "        except Exception:\n",
        "            dur, rms_db, peak_db = 0.0, -120.0, -120.0\n",
        "\n",
        "        return {\"utt_id\": row.get(\"utt_id\",\"\"), \"path_in\": pin, \"path_out\": str(pout),\n",
        "                \"profile\": PROFILE, \"duration\": round(dur,3),\n",
        "                \"rms_db\": round(rms_db,2), \"peak_db\": round(peak_db,2)}\n",
        "    except Exception as e:\n",
        "        try: tmp.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "        return {\"error\": str(e), \"path_in\": pin}\n",
        "\n",
        "# -------- Run --------\n",
        "planned = len(df_in)\n",
        "already = sum(1 for _ , r in df_in.iterrows() if already_done(str(r[\"path\"])))\n",
        "todo = planned - already\n",
        "log(f\"\\n[preproc:{PROFILE}] PLANNED={planned:,} | ALREADY_DONE={already:,} | TO_DO={todo:,}\")\n",
        "\n",
        "created = 0; errs = 0\n",
        "buf, errbuf = [], []\n",
        "t0 = time.time()\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    futs = [ex.submit(preprocess_one, r) for _, r in df_in.iterrows() if not already_done(str(r[\"path\"]))]\n",
        "    done_ct = 0\n",
        "    for f in as_completed(futs):\n",
        "        done_ct += 1\n",
        "        res = f.result()\n",
        "        if res is None:\n",
        "            pass\n",
        "        elif isinstance(res, dict) and \"error\" in res:\n",
        "            errs += 1\n",
        "            if len(errbuf) < 5: errbuf.append(res)  # capture first few errors for inspection\n",
        "        else:\n",
        "            buf.append(res)\n",
        "            created += 1\n",
        "\n",
        "        if done_ct % 500 == 0 or done_ct == todo:\n",
        "            print(f\"[{PROFILE}] {done_ct:,}/{todo:,}  (+{created} ok, {errs} err)  | elapsed {time.time()-t0:.1f}s\", flush=True)\n",
        "\n",
        "        if len(buf) >= BATCH_WRITE or (done_ct == todo and buf):\n",
        "            with open(MANIFEST, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
        "                w = csv.DictWriter(fcsv, fieldnames=[\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "                for row in buf: w.writerow(row)\n",
        "            buf.clear()\n",
        "\n",
        "print(f\"[{PROFILE}] FINISHED: +{created} files, {errs} errors | total elapsed {time.time()-t0:.1f}s\")\n",
        "if errbuf:\n",
        "    print(\"\\n--- First few errors (trimmed) ---\")\n",
        "    for e in errbuf:\n",
        "        print(e[\"path_in\"], \"->\", e[\"error\"])\n",
        "\n",
        "# -------- Summary --------\n",
        "def hours_from_manifest(profile: str):\n",
        "    try:\n",
        "        m = pd.read_csv(MANIFEST)\n",
        "        m = m[m[\"profile\"]==profile]\n",
        "        return float(pd.to_numeric(m[\"duration\"], errors=\"coerce\").fillna(0).sum()/3600.0), len(m)\n",
        "    except Exception:\n",
        "        return 0.0, 0\n",
        "\n",
        "h, n = hours_from_manifest(PROFILE)\n",
        "print(f\"[{PROFILE}] Manifest rows: {n:,} | hours: {h:.2f}h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Base Profile Audit**\n",
        "We verify that every processed file on disk is correctly indexed in `proc_manifest.csv`, backfilling any missing entries to ensure the dataset is complete."
      ],
      "metadata": {
        "id": "75hL0Cvocae2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVnW-1Ip2FkL",
        "outputId": "50076826-6188-43c5-9ba3-283f4002f2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[counts] profile=base | outputs_on_disk=94,789 | manifest_rows=94,789\n",
            "[diff] on-disk but not in manifest: 0\n",
            "[post] manifest_rows=94,789\n",
            "[ok] FAST audit/repair done.\n"
          ]
        }
      ],
      "source": [
        "# ================= FAST PREPROC AUDIT + REPAIR (no full disk-hours scan) =================\n",
        "from pathlib import Path\n",
        "import pandas as pd, csv\n",
        "import soundfile as sf\n",
        "\n",
        "ROOT      = Path(\"/content/drive/MyDrive/hindi_dfake\")\n",
        "META      = ROOT / \"metadata\"\n",
        "OUT_ROOT  = ROOT / \"processed\" / \"wav\"\n",
        "MANIFEST  = META / \"proc_manifest.csv\"\n",
        "PROFILE   = \"base\"   # change/add more profiles later if needed\n",
        "\n",
        "def read_csv_safe(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    for c in [\"utt_id\",\"path\",\"duration\",\"label\",\"fake_type\",\"source\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\" if c != \"duration\" else 0.0\n",
        "    return df\n",
        "\n",
        "def out_path_for(profile: str, in_path: str) -> Path:\n",
        "    p = Path(in_path)\n",
        "    try:    rel = p.relative_to(ROOT)\n",
        "    except: rel = Path(\"_external\") / p.name\n",
        "    return OUT_ROOT / profile / rel\n",
        "\n",
        "def in_path_from_out(profile: str, out_path: Path) -> Path:\n",
        "    base = OUT_ROOT / profile\n",
        "    try:\n",
        "        rel = out_path.relative_to(base)\n",
        "        return ROOT / rel\n",
        "    except Exception:\n",
        "        return ROOT / \"_unknown_source\" / out_path.name\n",
        "\n",
        "def fast_duration(path: Path) -> float:\n",
        "    try:\n",
        "        info = sf.info(str(path))\n",
        "        if info.samplerate and info.frames:\n",
        "            return float(info.frames) / float(info.samplerate)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "# Universe of inputs that actually exist\n",
        "dfs = [read_csv_safe(META/\"master_real.csv\"), read_csv_safe(META/\"master_fake.csv\")]\n",
        "att = META/\"attacks.labeled.csv\"\n",
        "if att.exists():\n",
        "    dfs.append(read_csv_safe(att))\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "df_all = df_all[df_all[\"path\"].astype(str).apply(lambda p: Path(p).exists())].copy()\n",
        "\n",
        "# Build lookup from input path -> row (for backfill metadata)\n",
        "in_index = df_all.set_index(\"path\").to_dict(\"index\")\n",
        "\n",
        "# Manifest slice\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST)\n",
        "else:\n",
        "    MANIFEST.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "    mf = pd.read_csv(MANIFEST)\n",
        "\n",
        "mf_p = mf[mf[\"profile\"] == PROFILE].copy()\n",
        "set_mani = set(mf_p[\"path_out\"].astype(str))\n",
        "\n",
        "# On-disk outputs for this profile\n",
        "disk_paths = list((OUT_ROOT / PROFILE).rglob(\"*.wav\"))\n",
        "set_disk = set(str(p) for p in disk_paths)\n",
        "\n",
        "print(f\"[counts] profile={PROFILE} | outputs_on_disk={len(set_disk):,} | manifest_rows={len(mf_p):,}\")\n",
        "\n",
        "missing_in_manifest = sorted(set_disk - set_mani)\n",
        "print(f\"[diff] on-disk but not in manifest: {len(missing_in_manifest):,}\")\n",
        "\n",
        "# ----- Backfill only the missing ones (fast) -----\n",
        "if missing_in_manifest:\n",
        "    rows = []\n",
        "    for out_str in missing_in_manifest:\n",
        "        outp = Path(out_str)\n",
        "        in_p = in_path_from_out(PROFILE, outp)\n",
        "        base = in_index.get(str(in_p), {})  # may be {}\n",
        "        dur = fast_duration(outp)\n",
        "\n",
        "        rows.append({\n",
        "            \"utt_id\": base.get(\"utt_id\",\"\"),\n",
        "            \"path_in\": str(in_p),\n",
        "            \"path_out\": out_str,\n",
        "            \"profile\": PROFILE,\n",
        "            \"duration\": round(dur,3),\n",
        "            \"rms_db\": \"\",   # unknown here (optional to compute later)\n",
        "            \"peak_db\": \"\",  # unknown here\n",
        "        })\n",
        "\n",
        "    with open(MANIFEST, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=[\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "        for r in rows: w.writerow(r)\n",
        "    print(f\"[fix] backfilled {len(rows):,} manifest rows.\")\n",
        "\n",
        "    # reload\n",
        "    mf = pd.read_csv(MANIFEST)\n",
        "    mf_p = mf[mf[\"profile\"] == PROFILE].copy()\n",
        "\n",
        "# ----- Dedup manifest by (profile, path_out) -----\n",
        "before = len(mf)\n",
        "mf = mf.drop_duplicates(subset=[\"profile\",\"path_out\"], keep=\"first\").reset_index(drop=True)\n",
        "after = len(mf)\n",
        "if after != before:\n",
        "    mf.to_csv(MANIFEST, index=False)\n",
        "    print(f\"[fix] dedup manifest: -{before-after} rows\")\n",
        "\n",
        "# Final counts\n",
        "mf_p = mf[mf[\"profile\"] == PROFILE].copy()\n",
        "print(f\"[post] manifest_rows={len(mf_p):,}\")\n",
        "print(\"[ok] FAST audit/repair done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Profile: Strong**\n",
        "We generate a robust training set by applying RawBoost (noise, reverb) and Artifact EQ (3-6kHz boost) to all files."
      ],
      "metadata": {
        "id": "jNkVhYJucm0R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "cdaf48336c6c43e7a1b654cb2244826d",
            "9f0e552babc449639623e2bf0ab6d6b5",
            "50fa984df09d4b77b9656f3dba022bcf",
            "3113c23696aa491fa844c74ca4070bce",
            "8901863d37674006a6ba27b95395b07d",
            "329c7da28bee42c0ba73bbde2070054d",
            "36db58c547ed430d84782aa5c1c21a16",
            "dfc3c49465e94650944531388d51742a",
            "590acd365d134fc69753bf2dfb35b5af",
            "22c1ecd8bd3d4714bfe066fc21e7a144",
            "4355513826d14a9e883a67553d6d649b"
          ]
        },
        "id": "23VypTgY8zIl",
        "outputId": "65667926-e2a7-4762-ed8f-c8d889c2cbf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input counts (normalized):\n",
            "label\n",
            "fake    45166\n",
            "real    49623\n",
            "\n",
            "[preproc:strong] PLANNED=94,789 | ALREADY_DONE=40 | TO_DO=94,749\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdaf48336c6c43e7a1b654cb2244826d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preproc:strong:   0%|          | 0/94749 [00:00<?, ?file/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[strong] FINISHED: +94749 files, 0 errors | elapsed 270.4 min\n",
            "[strong] Manifest rows: 94,789 | hours: 181.31 h\n"
          ]
        }
      ],
      "source": [
        "# ================= PREPROCESSING — STRONG (EQ + RawBoost v3) =================\n",
        "# Writes: /content/drive/MyDrive/hindi_dfake/processed/wav/strong/...\n",
        "# Logs to: /content/drive/MyDrive/hindi_dfake/metadata/proc_manifest.csv (profile=strong)\n",
        "\n",
        "import os, csv, uuid, time, hashlib, subprocess\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np, soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ------------ CONFIG ------------\n",
        "ROOT = Path(\"/content/drive/MyDrive/hindi_dfake\")\n",
        "META = ROOT / \"metadata\"\n",
        "OUT_ROOT = ROOT / \"processed\" / \"wav\"\n",
        "\n",
        "PROFILE = \"strong\"          # strong = base + artifact EQ + RawBoost v3\n",
        "INCLUDE_ATTACKS = True\n",
        "FRACTION_REAL = 1.00        # process all\n",
        "FRACTION_FAKE = 1.00\n",
        "HASH_SEED = 2025\n",
        "\n",
        "# Loudness & trim\n",
        "TARGET_DB   = -26.0\n",
        "TRIM_THR_DB = -45\n",
        "TRIM_DUR_S  = 0.20\n",
        "\n",
        "# Concurrency / batching\n",
        "MAX_WORKERS   = 6           # 6–8 is reasonable on Colab CPU\n",
        "INFLIGHT_MULT = 3\n",
        "BATCH_WRITE   = 500\n",
        "\n",
        "# RawBoost v3–style knobs\n",
        "RB_ENABLE           = True\n",
        "RB_BANDPASS_HZ      = (2500, 6000)\n",
        "RB_SNR_DB           = 25.0\n",
        "RB_NOISE_PROB       = 1.0\n",
        "RB_IMPULSE_PROB     = 0.5\n",
        "RB_IMPULSES_PER_SEC = 1.0\n",
        "RB_IMPULSE_GAIN     = 0.08\n",
        "RB_REVERB_PROB      = 0.6\n",
        "RB_REVERB_T_SEC     = 0.03\n",
        "RB_REVERB_DECAY     = 0.35\n",
        "RB_REVERB_WET       = 0.18\n",
        "\n",
        "REAL_CSV  = META / \"master_real.csv\"\n",
        "FAKE_CSV  = META / \"master_fake.csv\"\n",
        "ATT_CSV   = META / \"attacks.labeled.csv\"\n",
        "MANIFEST  = META / \"proc_manifest.csv\"\n",
        "assert REAL_CSV.exists() and FAKE_CSV.exists()\n",
        "\n",
        "# ------------ helpers ------------\n",
        "def read_csv_safe(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    for c in [\"utt_id\",\"path\",\"duration\",\"label\",\"fake_type\",\"source\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\" if c != \"duration\" else 0.0\n",
        "    return df\n",
        "\n",
        "def normalize_label(lab, fake_type, src, path):\n",
        "    l = str(lab).strip().lower()\n",
        "    if l in {\"real\",\"fake\"}: return l\n",
        "    ft = str(fake_type).strip().lower()\n",
        "    s  = str(src).strip().lower()\n",
        "    p  = str(path)\n",
        "    if \"/raw/real_clean\" in p or s.startswith(\"real_\"): return \"real\"\n",
        "    if ft in {\"tts_edge\",\"channel_attack\",\"vc\"} or \"/raw/fake_\" in p or s.startswith(\"fake_\"): return \"fake\"\n",
        "    return \"fake\"\n",
        "\n",
        "def stable_frac_mask(series: pd.Series, frac: float, seed: int) -> pd.Series:\n",
        "    if frac >= 1.0: return pd.Series([True]*len(series), index=series.index)\n",
        "    s = series.astype(str).apply(lambda p: int(hashlib.sha1((p+str(seed)).encode()).hexdigest()[:8], 16) / 0xFFFFFFFF)\n",
        "    return s < frac\n",
        "\n",
        "def out_path_for(in_path: str) -> Path:\n",
        "    p = Path(in_path)\n",
        "    try:    rel = p.relative_to(ROOT)\n",
        "    except: rel = Path(\"_external\") / p.name\n",
        "    return OUT_ROOT / PROFILE / rel\n",
        "\n",
        "def ensure_parent(p: Path): p.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def rms_dbfs_arr(x: np.ndarray) -> float:\n",
        "    if x.ndim > 1: x = x.mean(axis=1)\n",
        "    if len(x) == 0: return -120.0\n",
        "    rms = float(np.sqrt(np.mean(np.square(x))))\n",
        "    if rms <= 1e-9: return -120.0\n",
        "    return 20.0*np.log10(np.clip(rms, 1e-9, 1.0))\n",
        "\n",
        "# ---- RawBoost v3–style (deterministic per file) ----\n",
        "def _rng_from_key(key: str):\n",
        "    seed = int(hashlib.sha1(key.encode(\"utf-8\")).hexdigest()[:8], 16)\n",
        "    return np.random.default_rng(seed)\n",
        "\n",
        "def _add_colored_noise(x, sr, rng, band=(2500,6000), snr_db=25.0):\n",
        "    n = len(x)\n",
        "    if n == 0: return x\n",
        "    wn = rng.standard_normal(n).astype(np.float32)\n",
        "    X = np.fft.rfft(wn); freqs = np.fft.rfftfreq(n, d=1.0/sr)\n",
        "    mask = (freqs >= band[0]) & (freqs <= band[1]); X[~mask] = 0.0\n",
        "    noise = np.fft.irfft(X, n=n).astype(np.float32)\n",
        "    sig_rms = np.sqrt(np.mean(np.square(x))) + 1e-9\n",
        "    target_noise_rms = sig_rms / (10.0**(snr_db/20.0))\n",
        "    cur_rms = np.sqrt(np.mean(np.square(noise))) + 1e-12\n",
        "    noise *= (target_noise_rms / cur_rms)\n",
        "    return np.clip(x + noise, -1.0, 1.0)\n",
        "\n",
        "def _add_impulses(x, sr, rng, per_sec=1.0, gain=0.08):\n",
        "    n = len(x)\n",
        "    if n == 0: return x\n",
        "    dur = n/float(sr); k = max(1, int(per_sec * dur))\n",
        "    idx = rng.integers(0, n, size=k)\n",
        "    amp = gain * (np.sqrt(np.mean(np.square(x))) + 1e-9)\n",
        "    y = x.copy()\n",
        "    y[idx] = np.clip(y[idx] + amp * rng.choice([-1.0, 1.0], size=k), -1.0, 1.0)\n",
        "    return y\n",
        "\n",
        "def _add_small_reverb(x, sr, rng, t_sec=0.03, decay=0.35, wet=0.18):\n",
        "    n = len(x)\n",
        "    if n == 0: return x\n",
        "    ir_len = max(8, int(t_sec * sr))\n",
        "    t = np.arange(ir_len, dtype=np.float32)\n",
        "    ir = np.exp(-decay * t / ir_len).astype(np.float32)\n",
        "    for _ in range(3):\n",
        "        pos = int(rng.integers(0, ir_len))\n",
        "        ir[pos] += float(rng.uniform(0.1, 0.3))\n",
        "    ir /= (np.sum(np.abs(ir)) + 1e-9)\n",
        "    y = np.convolve(x, ir, mode=\"full\")[:n].astype(np.float32)\n",
        "    return np.clip((1.0 - wet) * x + wet * y, -1.0, 1.0)\n",
        "\n",
        "def rawboost_v3(x: np.ndarray, sr: int, key: str) -> np.ndarray:\n",
        "    if x.ndim > 1: x = x.mean(axis=1)\n",
        "    x = np.clip(x, -1.0, 1.0).astype(np.float32)\n",
        "    if not RB_ENABLE: return x\n",
        "    rng = _rng_from_key(key)\n",
        "    if rng.uniform() < RB_NOISE_PROB:\n",
        "        x = _add_colored_noise(x, sr, rng, RB_BANDPASS_HZ, RB_SNR_DB)\n",
        "    if rng.uniform() < RB_IMPULSE_PROB:\n",
        "        x = _add_impulses(x, sr, rng, RB_IMPULSES_PER_SEC, RB_IMPULSE_GAIN)\n",
        "    if rng.uniform() < RB_REVERB_PROB:\n",
        "        x = _add_small_reverb(x, sr, rng, RB_REVERB_T_SEC, RB_REVERB_DECAY, RB_REVERB_WET)\n",
        "    return x\n",
        "\n",
        "def build_filter_chain_strong(gain_db: float) -> str:\n",
        "    return \",\".join([\n",
        "        \"highpass=f=20\",\n",
        "        \"equalizer=f=3000:t=q:w=1.0:g=2.5\",\n",
        "        \"equalizer=f=4800:t=q:w=0.9:g=2.0\",\n",
        "        \"treble=g=1.0:f=6000:t=h:w=0.7\",\n",
        "        f\"volume={gain_db}dB\",\n",
        "        f\"silenceremove=start_periods=1:start_duration={TRIM_DUR_S}:start_threshold={TRIM_THR_DB}dB:stop_periods=1:stop_duration={TRIM_DUR_S}:stop_threshold={TRIM_THR_DB}dB\"\n",
        "    ])\n",
        "\n",
        "# ------------ load inputs ------------\n",
        "dfR = read_csv_safe(REAL_CSV)\n",
        "dfF = read_csv_safe(FAKE_CSV)\n",
        "dfs = [dfR, dfF]\n",
        "if INCLUDE_ATTACKS and ATT_CSV.exists():\n",
        "    dfs.append(read_csv_safe(ATT_CSV))\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "df_all = df_all[df_all[\"path\"].astype(str).apply(lambda p: Path(p).exists())].copy()\n",
        "df_all[\"label\"] = [normalize_label(l, ft, s, p) for l, ft, s, p in\n",
        "                   zip(df_all[\"label\"], df_all[\"fake_type\"], df_all[\"source\"], df_all[\"path\"])]\n",
        "\n",
        "# stratified deterministic subset (here: all)\n",
        "parts = []\n",
        "for lab, g in df_all.groupby(df_all[\"label\"].astype(str).str.lower()):\n",
        "    frac = FRACTION_REAL if lab == \"real\" else FRACTION_FAKE\n",
        "    parts.append(g if frac>=1.0 else g[stable_frac_mask(g[\"path\"], frac, HASH_SEED)])\n",
        "df_in = pd.concat(parts, ignore_index=True) if parts else df_all.copy()\n",
        "\n",
        "# ------------ manifest skip logic ------------\n",
        "MANIFEST.parent.mkdir(parents=True, exist_ok=True)\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST, usecols=[\"profile\",\"path_out\"])\n",
        "    done_set = set(mf.loc[mf[\"profile\"]==PROFILE, \"path_out\"].astype(str))\n",
        "else:\n",
        "    with open(MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "    done_set = set()\n",
        "\n",
        "def already_done(pin: str) -> bool:\n",
        "    pout = out_path_for(pin)\n",
        "    return pout.exists() or (str(pout) in done_set)\n",
        "\n",
        "todo_rows = [r for _, r in df_in.iterrows() if not already_done(str(r[\"path\"]))]\n",
        "planned, todo = len(df_in), len(todo_rows)\n",
        "print(f\"Input counts (normalized):\")\n",
        "print(df_all.groupby(df_all['label']).size().to_string())\n",
        "print(f\"\\n[preproc:{PROFILE}] PLANNED={planned:,} | ALREADY_DONE={planned-todo:,} | TO_DO={todo:,}\")\n",
        "\n",
        "# ------------ worker ------------\n",
        "def preprocess_one(row):\n",
        "    pin = str(row[\"path\"])\n",
        "    pout = out_path_for(pin)\n",
        "    ensure_parent(pout)\n",
        "\n",
        "    # read (and resample if needed)\n",
        "    try:\n",
        "        x, sr = sf.read(pin, dtype=\"float32\", always_2d=False)\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"read_fail: {e}\", \"path_in\": pin}\n",
        "    if x.ndim > 1: x = x.mean(axis=1)\n",
        "    if sr != 16000:\n",
        "        tmp = pout.parent / f\".{pout.stem}.in-{uuid.uuid4().hex}.wav\"\n",
        "        cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "               \"-i\", pin, \"-ac\",\"1\",\"-ar\",\"16000\",\"-sample_fmt\",\"s16\", str(tmp)]\n",
        "        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        if p.returncode != 0:\n",
        "            try: tmp.unlink(missing_ok=True)\n",
        "            except: pass\n",
        "            return {\"error\": \"resample_fail\", \"path_in\": pin}\n",
        "        x, sr = sf.read(str(tmp), dtype=\"float32\", always_2d=False)\n",
        "        try: tmp.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "\n",
        "    x = np.clip(x, -1.0, 1.0).astype(np.float32)\n",
        "\n",
        "    # RawBoost\n",
        "    key = f\"{pin}|{PROFILE}\"\n",
        "    try:    y = rawboost_v3(x, sr, key)\n",
        "    except: y = x\n",
        "\n",
        "    # write RB temp near target\n",
        "    tmp_rb = pout.parent / f\".{pout.stem}.rb-{uuid.uuid4().hex}.wav\"\n",
        "    sf.write(str(tmp_rb), y, 16000, subtype=\"PCM_16\")\n",
        "\n",
        "    # EQ + loudness + trim via ffmpeg\n",
        "    gain_db = float(np.clip(TARGET_DB - rms_dbfs_arr(y), -20.0, 20.0))\n",
        "    FILTER  = build_filter_chain_strong(gain_db)\n",
        "\n",
        "    tmp_out = pout.parent / f\".{pout.stem}.tmp-{uuid.uuid4().hex}.wav\"\n",
        "    cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "           \"-i\", str(tmp_rb), \"-ac\",\"1\",\"-ar\",\"16000\",\n",
        "           \"-af\", FILTER, \"-sample_fmt\",\"s16\", str(tmp_out)]\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    try: tmp_rb.unlink(missing_ok=True)\n",
        "    except: pass\n",
        "    if p.returncode != 0:\n",
        "        try: tmp_out.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "        return {\"error\": p.stderr.decode(\"utf-8\",\"ignore\")[:300], \"path_in\": pin}\n",
        "\n",
        "    os.replace(str(tmp_out), str(pout))\n",
        "\n",
        "    # stats\n",
        "    try:\n",
        "        z, sr2 = sf.read(str(pout), dtype=\"float32\", always_2d=False)\n",
        "        if z.ndim > 1: z = z.mean(axis=1)\n",
        "        dur = len(z)/float(sr2) if sr2 else 0.0\n",
        "        peak = float(np.max(np.abs(z))) if len(z) else 0.0\n",
        "        rms  = float(np.sqrt(np.mean(np.square(z)))) if len(z) else 0.0\n",
        "        rms_db = -120.0 if rms<=1e-9 else 20*np.log10(np.clip(rms,1e-9,1.0))\n",
        "        peak_db = -120.0 if peak<=1e-9 else 20*np.log10(np.clip(peak,1e-9,1.0))\n",
        "    except Exception:\n",
        "        dur, rms_db, peak_db = 0.0, -120.0, -120.0\n",
        "\n",
        "    return {\"utt_id\": row.get(\"utt_id\",\"\"), \"path_in\": pin, \"path_out\": str(pout),\n",
        "            \"profile\": PROFILE, \"duration\": round(dur,3),\n",
        "            \"rms_db\": round(rms_db,2), \"peak_db\": round(peak_db,2)}\n",
        "\n",
        "# ------------ streaming scheduler with progress ------------\n",
        "created = 0; errs = 0; buf = []\n",
        "start = time.time()\n",
        "inflight_cap = max(MAX_WORKERS * INFLIGHT_MULT, MAX_WORKERS)\n",
        "pbar = tqdm(total=len(todo_rows), desc=f\"preproc:{PROFILE}\", unit=\"file\")\n",
        "\n",
        "def flush_manifest(rows):\n",
        "    if not rows: return\n",
        "    with open(MANIFEST, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
        "        w = csv.DictWriter(fcsv, fieldnames=[\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "        for r in rows: w.writerow(r)\n",
        "\n",
        "it = iter(todo_rows)\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    inflight = set()\n",
        "    for _ in range(min(inflight_cap, len(todo_rows))):\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: break\n",
        "    while inflight:\n",
        "        done = next(as_completed(inflight))\n",
        "        inflight.remove(done)\n",
        "        res = done.result()\n",
        "        pbar.update(1)\n",
        "        if isinstance(res, dict) and \"error\" in res:\n",
        "            errs += 1\n",
        "        elif res:\n",
        "            buf.append(res); created += 1\n",
        "            if len(buf) >= BATCH_WRITE:\n",
        "                flush_manifest(buf); buf.clear()\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: pass\n",
        "\n",
        "flush_manifest(buf)\n",
        "pbar.close()\n",
        "print(f\"\\n[{PROFILE}] FINISHED: +{created} files, {errs} errors | elapsed {(time.time()-start)/60:.1f} min\")\n",
        "\n",
        "# quick manifest slice\n",
        "if MANIFEST.exists():\n",
        "    m = pd.read_csv(MANIFEST)\n",
        "    mp = m[m[\"profile\"]==PROFILE]\n",
        "    hrs = float(pd.to_numeric(mp[\"duration\"], errors=\"coerce\").fillna(0).sum()/3600.0)\n",
        "    print(f\"[{PROFILE}] Manifest rows: {len(mp):,} | hours: {hrs:.2f} h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Strong Profile Audit**\n",
        "We verify the integrity of the augmented dataset, confirming that all files are present, correctly formatted (16kHz PCM_16), and indexed in the manifest."
      ],
      "metadata": {
        "id": "o9DuGnF8crEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqNVlFd19RXQ",
        "outputId": "3027f0da-25ae-4f42-e0b2-1916b40bbe9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Universe] inputs that exist: 94789\n",
            "[counts:strong] expected=94,789 | on_disk=94,789 | manifest_rows=94,789\n",
            "[diff] expected but missing on disk: 0\n",
            "[diff] on disk but missing in manifest: 0\n",
            "[diff] manifest rows without file: 0\n",
            "[probe] sample=300 | sr={16000: 300} | ch={1: 300} | subtype={'PCM_16': 300} | bad=0\n",
            "[hours:strong] rows=94,789 | hours=181.31h\n",
            "\n",
            "[OK] Audit complete.\n"
          ]
        }
      ],
      "source": [
        "# ===================== STRONG: one-shot sanity audit =====================\n",
        "from pathlib import Path\n",
        "import pandas as pd, csv, random\n",
        "import soundfile as sf\n",
        "\n",
        "ROOT      = Path(\"/content/drive/MyDrive/hindi_dfake\")\n",
        "META      = ROOT / \"metadata\"\n",
        "OUT_ROOT  = ROOT / \"processed\" / \"wav\"\n",
        "PROFILE   = \"strong\"\n",
        "\n",
        "REAL_CSV  = META / \"master_real.csv\"\n",
        "FAKE_CSV  = META / \"master_fake.csv\"\n",
        "ATT_CSV   = META / \"attacks.labeled.csv\"\n",
        "MANIFEST  = META / \"proc_manifest.csv\"\n",
        "\n",
        "def read_csv_safe(p):\n",
        "    df = pd.read_csv(p)\n",
        "    for c in [\"utt_id\",\"path\",\"duration\",\"label\",\"fake_type\",\"source\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\" if c != \"duration\" else 0.0\n",
        "    return df\n",
        "\n",
        "def normalize_label(lab, fake_type, src, path):\n",
        "    l = str(lab).strip().lower()\n",
        "    if l in {\"real\",\"fake\"}: return l\n",
        "    ft = str(fake_type).strip().lower()\n",
        "    s  = str(src).strip().lower()\n",
        "    p  = str(path)\n",
        "    if \"/raw/real_clean\" in p or s.startswith(\"real_\"): return \"real\"\n",
        "    if ft in {\"tts_edge\",\"channel_attack\",\"vc\"} or \"/raw/fake_\" in p or s.startswith(\"fake_\"): return \"fake\"\n",
        "    return \"fake\"\n",
        "\n",
        "def out_path_for(in_path: str) -> Path:\n",
        "    p = Path(in_path)\n",
        "    try:\n",
        "        rel = p.relative_to(ROOT)\n",
        "    except Exception:\n",
        "        rel = Path(\"_external\") / p.name\n",
        "    return OUT_ROOT / PROFILE / rel\n",
        "\n",
        "# ---------- load inputs (only those that exist) ----------\n",
        "dfs = [read_csv_safe(REAL_CSV), read_csv_safe(FAKE_CSV)]\n",
        "if ATT_CSV.exists():\n",
        "    dfs.append(read_csv_safe(ATT_CSV))\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "df_all = df_all[df_all[\"path\"].astype(str).apply(lambda p: Path(p).exists())].copy()\n",
        "df_all[\"label\"] = [normalize_label(l, ft, s, p) for l, ft, s, p in\n",
        "                   zip(df_all[\"label\"], df_all[\"fake_type\"], df_all[\"source\"], df_all[\"path\"])]\n",
        "\n",
        "expected = set(str(out_path_for(p)) for p in df_all[\"path\"].astype(str))\n",
        "\n",
        "# ---------- what’s on disk & in manifest ----------\n",
        "on_disk  = set(str(p) for p in (OUT_ROOT/PROFILE).rglob(\"*.wav\"))\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST)\n",
        "else:\n",
        "    MANIFEST.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "    mf = pd.read_csv(MANIFEST)\n",
        "\n",
        "mf_p     = mf[mf[\"profile\"]==PROFILE]\n",
        "in_mani  = set(mf_p[\"path_out\"].astype(str))\n",
        "\n",
        "print(\"[Universe] inputs that exist:\", len(df_all))\n",
        "print(f\"[counts:{PROFILE}] expected={len(expected):,} | on_disk={len(on_disk):,} | manifest_rows={len(in_mani):,}\")\n",
        "\n",
        "miss_disk   = expected - on_disk\n",
        "extra_disk  = on_disk - expected\n",
        "miss_mani   = on_disk  - in_mani\n",
        "extra_mani  = in_mani  - on_disk\n",
        "\n",
        "print(f\"[diff] expected but missing on disk: {len(miss_disk):,}\")\n",
        "print(f\"[diff] on disk but missing in manifest: {len(miss_mani):,}\")\n",
        "print(f\"[diff] manifest rows without file: {len(extra_mani):,}\")\n",
        "\n",
        "# ---------- backfill manifest if needed ----------\n",
        "def fast_duration(p):\n",
        "    try:\n",
        "        info = sf.info(p)\n",
        "        return round(info.frames / info.samplerate, 3)\n",
        "    except: return 0.0\n",
        "\n",
        "if len(miss_mani) > 0:\n",
        "    print(f\"[fix] backfilling {len(miss_mani):,} manifest rows …\")\n",
        "    rows = []\n",
        "    # rough reconstruction of input path by mirroring\n",
        "    for out_str in list(miss_mani):\n",
        "        outp = Path(out_str)\n",
        "        try:\n",
        "            rel = outp.relative_to(OUT_ROOT/PROFILE)\n",
        "            in_p = ROOT / rel\n",
        "        except Exception:\n",
        "            in_p = ROOT / \"_unknown_source\" / outp.name\n",
        "        rows.append({\n",
        "            \"utt_id\": \"\", \"path_in\": str(in_p), \"path_out\": out_str,\n",
        "            \"profile\": PROFILE, \"duration\": fast_duration(str(outp)),\n",
        "            \"rms_db\": \"\", \"peak_db\": \"\"\n",
        "        })\n",
        "    with open(MANIFEST, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=[\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "        for r in rows: w.writerow(r)\n",
        "    print(\"[fix] backfill done.\")\n",
        "\n",
        "# ---------- dedup manifest on (profile, path_out) ----------\n",
        "mf2 = pd.read_csv(MANIFEST)\n",
        "before = len(mf2)\n",
        "mf2 = mf2.drop_duplicates(subset=[\"profile\",\"path_out\"], keep=\"first\").reset_index(drop=True)\n",
        "if len(mf2) != before:\n",
        "    mf2.to_csv(MANIFEST, index=False)\n",
        "    print(f\"[fix] dedup manifest: -{before - len(mf2)} rows\")\n",
        "\n",
        "# ---------- spot-check audio format on a random sample ----------\n",
        "sample_n = min(300, len(on_disk))\n",
        "sample_paths = random.sample(list(on_disk), sample_n) if sample_n > 0 else []\n",
        "sr_counts, ch_counts, subtype_counts, bad = {}, {}, {}, 0\n",
        "for p in sample_paths:\n",
        "    try:\n",
        "        info = sf.info(p)\n",
        "        sr_counts[info.samplerate] = sr_counts.get(info.samplerate, 0) + 1\n",
        "        ch_counts[info.channels]   = ch_counts.get(info.channels,   0) + 1\n",
        "        subtype_counts[info.subtype] = subtype_counts.get(info.subtype, 0) + 1\n",
        "    except Exception:\n",
        "        bad += 1\n",
        "\n",
        "print(f\"[probe] sample={sample_n} | sr={sr_counts} | ch={ch_counts} | subtype={subtype_counts} | bad={bad}\")\n",
        "\n",
        "# ---------- hours from manifest (profile=strong) ----------\n",
        "mf = pd.read_csv(MANIFEST)\n",
        "ms = mf[mf[\"profile\"]==PROFILE]\n",
        "hours = float(pd.to_numeric(ms[\"duration\"], errors=\"coerce\").fillna(0).sum()/3600.0)\n",
        "print(f\"[hours:{PROFILE}] rows={len(ms):,} | hours={hours:.2f}h\")\n",
        "\n",
        "print(\"\\n[OK] Audit complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Test Set Integration (Fake) - Base Profile**\n",
        "We re-run the `base` processor to catch newly added files (specifically the fake test set). The script detects the 3,004 new files and processes them without re-doing the existing ~94k."
      ],
      "metadata": {
        "id": "WOL2gi98LUjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379,
          "referenced_widgets": [
            "557988d493a5426cb8530dcb05f77b96",
            "8c4a03e6979544bd83696c9848b6068f",
            "b27722ba0f434b50bc1052a5e9dc71e7",
            "09686df0258242b68e5ad9c90700f25f",
            "d49ddb72a4184115a43ca2da0c60ee76",
            "aa7ef1d6f84a41c295b071b1a40a7578",
            "c6754376b8394c63b5b5d0fe92672867",
            "27b0db3d99b146b385ffbe40e137c411",
            "eca99ad4ea3e4d55b38f4b134eb10362",
            "b0ba723ab2da46498195dd8b7af6746b",
            "677cc5d0aa3f437ea4d6cd1caa830939"
          ]
        },
        "id": "8MLr6TRZkGFP",
        "outputId": "76b14903-faca-4a0d-ba0a-f6d0f07a5912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input counts (normalized):\n",
            "label\n",
            "fake    48170\n",
            "real    49623\n",
            "\n",
            "[preproc:base] PLANNED=97,793 | ALREADY_DONE=94,789 | TO_DO=3,004\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "557988d493a5426cb8530dcb05f77b96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preproc:base:   0%|          | 0/3004 [00:00<?, ?file/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[base] FINISHED: +3004 files, 0 errors | elapsed 9.0 min\n",
            "[base] outputs on disk: 97,793 (showing a few)\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_6859d4f039aea68f.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_19e1c8d7cad439c0.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_a5ea2af5277b57c4.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_7aee2f81107fe9de.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_c103d799a208b3c8.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_8883f29f2fb7bfdf.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_e43afa0cec72f0aa.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_47ea9317998de3aa.wav\n",
            "[base] manifest rows: 97,793 | hours: 78.72 h\n",
            "[OK] BASE preprocessing done.\n"
          ]
        }
      ],
      "source": [
        "# ========== PREPROCESSING (BASE profile; resumable; same style as before) ==========\n",
        "# Pipeline: DC high-pass -> per-file gain to TARGET_DB -> light trim\n",
        "# Writes: /content/drive/MyDrive/hindi_dfake/processed/wav/base/...\n",
        "# Logs  : /content/drive/MyDrive/hindi_dfake/metadata/proc_manifest.csv (profile=base)\n",
        "\n",
        "import os, csv, time, uuid, hashlib, subprocess\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np, soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "ROOT = Path(\"/content/drive/MyDrive/hindi_dfake\")\n",
        "META = ROOT / \"metadata\"\n",
        "OUT_ROOT = ROOT / \"processed\" / \"wav\"\n",
        "\n",
        "PROFILE = \"base\"          # <- BASE profile\n",
        "INCLUDE_ATTACKS = True    # include attacks.labeled.csv if present\n",
        "FRACTION_REAL = 1.00      # process all (resumable)\n",
        "FRACTION_FAKE = 1.00\n",
        "HASH_SEED = 2025\n",
        "\n",
        "# Loudness & trim (same as your earlier cells)\n",
        "TARGET_DB   = -26.0       # target RMS (dBFS-ish)\n",
        "TRIM_THR_DB = -45\n",
        "TRIM_DUR_S  = 0.20\n",
        "\n",
        "# Concurrency / batching\n",
        "MAX_WORKERS   = 6\n",
        "INFLIGHT_MULT = 3\n",
        "BATCH_WRITE   = 500\n",
        "\n",
        "REAL_CSV  = META / \"master_real.csv\"\n",
        "FAKE_CSV  = META / \"master_fake.csv\"\n",
        "ATT_CSV   = META / \"attacks.labeled.csv\"\n",
        "MANIFEST  = META / \"proc_manifest.csv\"\n",
        "assert REAL_CSV.exists() and FAKE_CSV.exists(), \"Missing master_real.csv/master_fake.csv\"\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def read_csv_safe(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    for c in [\"utt_id\",\"path\",\"duration\",\"label\",\"fake_type\",\"source\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\" if c != \"duration\" else 0.0\n",
        "    return df\n",
        "\n",
        "def normalize_label(lab, fake_type, src, path):\n",
        "    l = str(lab).strip().lower()\n",
        "    if l in {\"real\",\"fake\"}: return l\n",
        "    ft = str(fake_type).strip().lower()\n",
        "    s  = str(src).strip().lower()\n",
        "    p  = str(path)\n",
        "    if \"/raw/real_clean\" in p or s.startswith(\"real_\"): return \"real\"\n",
        "    if ft in {\"tts_edge\",\"channel_attack\",\"vc\"} or \"/raw/fake_\" in p or s.startswith(\"fake_\"): return \"fake\"\n",
        "    return \"fake\"\n",
        "\n",
        "def stable_frac_mask(series: pd.Series, frac: float, seed: int) -> pd.Series:\n",
        "    if frac >= 1.0: return pd.Series([True]*len(series), index=series.index)\n",
        "    s = series.astype(str).apply(lambda p: int(hashlib.sha1((p+str(seed)).encode()).hexdigest()[:8], 16) / 0xFFFFFFFF)\n",
        "    return s < frac\n",
        "\n",
        "def out_path_for(in_path: str) -> Path:\n",
        "    p = Path(in_path)\n",
        "    try:    rel = p.relative_to(ROOT)\n",
        "    except: rel = Path(\"_external\") / p.name\n",
        "    return OUT_ROOT / PROFILE / rel\n",
        "\n",
        "def ensure_parent(p: Path): p.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def rms_dbfs(p: Path) -> float:\n",
        "    try:\n",
        "        x, sr = sf.read(str(p), dtype=\"float32\", always_2d=False)\n",
        "        if x.ndim > 1: x = x.mean(axis=1)\n",
        "        if len(x) == 0: return -120.0\n",
        "        rms = float(np.sqrt(np.mean(np.square(x))))\n",
        "        if rms <= 1e-9: return -120.0\n",
        "        return 20.0*np.log10(np.clip(rms, 1e-9, 1.0))\n",
        "    except Exception:\n",
        "        return -120.0\n",
        "\n",
        "def build_filter_chain_base(gain_db: float) -> str:\n",
        "    return \",\".join([\n",
        "        \"highpass=f=20\",\n",
        "        f\"volume={gain_db}dB\",\n",
        "        f\"silenceremove=start_periods=1:start_duration={TRIM_DUR_S}:start_threshold={TRIM_THR_DB}dB:\"\n",
        "        f\"stop_periods=1:stop_duration={TRIM_DUR_S}:stop_threshold={TRIM_THR_DB}dB\"\n",
        "    ])\n",
        "\n",
        "# ---------------- load inputs ----------------\n",
        "dfR = read_csv_safe(REAL_CSV)\n",
        "dfF = read_csv_safe(FAKE_CSV)\n",
        "dfs = [dfR, dfF]\n",
        "if INCLUDE_ATTACKS and ATT_CSV.exists(): dfs.append(read_csv_safe(ATT_CSV))\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "df_all = df_all[df_all[\"path\"].astype(str).apply(lambda p: Path(p).exists())].copy()\n",
        "df_all[\"label\"] = [normalize_label(l, ft, s, p) for l, ft, s, p in\n",
        "                   zip(df_all[\"label\"], df_all[\"fake_type\"], df_all[\"source\"], df_all[\"path\"])]\n",
        "\n",
        "print(\"Input counts (normalized):\")\n",
        "print(df_all.groupby(df_all[\"label\"].astype(str).str.lower()).size().to_string())\n",
        "\n",
        "# deterministic per-label subset (here: everything; still resumable)\n",
        "parts = []\n",
        "for lab, g in df_all.groupby(df_all[\"label\"].astype(str).str.lower()):\n",
        "    frac = FRACTION_REAL if lab == \"real\" else FRACTION_FAKE\n",
        "    parts.append(g if frac>=1.0 else g[stable_frac_mask(g[\"path\"], frac, HASH_SEED)])\n",
        "df_in = pd.concat(parts, ignore_index=True) if parts else df_all.copy()\n",
        "\n",
        "# ---------------- manifest / resume ----------------\n",
        "MANIFEST.parent.mkdir(parents=True, exist_ok=True)\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST, usecols=[\"profile\",\"path_out\"])\n",
        "    done_set = set(mf.loc[mf[\"profile\"]==PROFILE, \"path_out\"].astype(str))\n",
        "else:\n",
        "    with open(MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "    done_set = set()\n",
        "\n",
        "def already_done(pin: str) -> bool:\n",
        "    pout = out_path_for(pin)\n",
        "    return pout.exists() or (str(pout) in done_set)\n",
        "\n",
        "todo_rows = [r for _, r in df_in.iterrows() if not already_done(str(r[\"path\"]))]\n",
        "planned, todo = len(df_in), len(todo_rows)\n",
        "print(f\"\\n[preproc:{PROFILE}] PLANNED={planned:,} | ALREADY_DONE={planned-todo:,} | TO_DO={todo:,}\")\n",
        "\n",
        "# ---------------- worker (BASE chain) ----------------\n",
        "def preprocess_one(row):\n",
        "    pin = str(row[\"path\"])\n",
        "    pout = out_path_for(pin)\n",
        "    ensure_parent(pout)\n",
        "\n",
        "    # If input SR != 16k, resample first (same FS temp)\n",
        "    try:\n",
        "        x, sr = sf.read(pin, dtype=\"float32\", always_2d=False)\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"read_fail: {e}\", \"path_in\": pin}\n",
        "    if sr != 16000 or (x.ndim > 1):\n",
        "        tmp_in = pout.parent / f\".{pout.stem}.in-{uuid.uuid4().hex}.wav\"\n",
        "        cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "               \"-i\", pin, \"-ac\",\"1\",\"-ar\",\"16000\",\"-sample_fmt\",\"s16\", str(tmp_in)]\n",
        "        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        if p.returncode != 0:\n",
        "            try: tmp_in.unlink(missing_ok=True)\n",
        "            except: pass\n",
        "            return {\"error\": \"resample_fail\", \"path_in\": pin}\n",
        "        pin_for_chain = str(tmp_in)\n",
        "    else:\n",
        "        pin_for_chain = pin\n",
        "\n",
        "    gain_db = float(np.clip(TARGET_DB - rms_dbfs(Path(pin_for_chain)), -20.0, 20.0))\n",
        "    FILTER  = build_filter_chain_base(gain_db)\n",
        "\n",
        "    tmp_out = pout.parent / f\".{pout.stem}.tmp-{uuid.uuid4().hex}.wav\"\n",
        "    cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "           \"-i\", pin_for_chain, \"-ac\",\"1\",\"-ar\",\"16000\",\n",
        "           \"-af\", FILTER, \"-sample_fmt\",\"s16\", str(tmp_out)]\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    # clean temp (if created)\n",
        "    if pin_for_chain != pin:\n",
        "        try: Path(pin_for_chain).unlink(missing_ok=True)\n",
        "        except: pass\n",
        "\n",
        "    if p.returncode != 0:\n",
        "        try: tmp_out.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "        return {\"error\": p.stderr.decode(\"utf-8\",\"ignore\")[:300], \"path_in\": pin}\n",
        "\n",
        "    os.replace(str(tmp_out), str(pout))\n",
        "\n",
        "    # quick stats\n",
        "    try:\n",
        "        z, sr2 = sf.read(str(pout), dtype=\"float32\", always_2d=False)\n",
        "        if z.ndim > 1: z = z.mean(axis=1)\n",
        "        dur = len(z)/float(sr2) if sr2 else 0.0\n",
        "        peak = float(np.max(np.abs(z))) if len(z) else 0.0\n",
        "        rms  = float(np.sqrt(np.mean(np.square(z)))) if len(z) else 0.0\n",
        "        rms_db = -120.0 if rms<=1e-9 else 20*np.log10(np.clip(rms,1e-9,1.0))\n",
        "        peak_db = -120.0 if peak<=1e-9 else 20*np.log10(np.clip(peak,1e-9,1.0))\n",
        "    except Exception:\n",
        "        dur, rms_db, peak_db = 0.0, -120.0, -120.0\n",
        "\n",
        "    return {\"utt_id\": row.get(\"utt_id\",\"\"), \"path_in\": pin, \"path_out\": str(pout),\n",
        "            \"profile\": PROFILE, \"duration\": round(dur,3),\n",
        "            \"rms_db\": round(rms_db,2), \"peak_db\": round(peak_db,2)}\n",
        "\n",
        "# ---------------- streaming scheduler + progress ----------------\n",
        "created = 0; errs = 0; buf = []\n",
        "start = time.time()\n",
        "inflight_cap = max(MAX_WORKERS * INFLIGHT_MULT, MAX_WORKERS)\n",
        "pbar = tqdm(total=len(todo_rows), desc=f\"preproc:{PROFILE}\", unit=\"file\")\n",
        "\n",
        "def flush_manifest(rows):\n",
        "    if not rows: return\n",
        "    with open(MANIFEST, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
        "        w = csv.DictWriter(fcsv, fieldnames=[\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "        for r in rows: w.writerow(r)\n",
        "\n",
        "it = iter(todo_rows)\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    inflight = set()\n",
        "    for _ in range(min(inflight_cap, len(todo_rows))):\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: break\n",
        "    while inflight:\n",
        "        done = next(as_completed(inflight))\n",
        "        inflight.remove(done)\n",
        "        res = done.result()\n",
        "        pbar.update(1)\n",
        "        if isinstance(res, dict) and \"error\" in res:\n",
        "            errs += 1\n",
        "        elif res:\n",
        "            buf.append(res); created += 1\n",
        "            if len(buf) >= BATCH_WRITE:\n",
        "                flush_manifest(buf); buf.clear()\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: pass\n",
        "\n",
        "flush_manifest(buf)\n",
        "pbar.close()\n",
        "print(f\"\\n[{PROFILE}] FINISHED: +{created} files, {errs} errors | elapsed {(time.time()-start)/60:.1f} min\")\n",
        "\n",
        "# ---------------- quick audit / summary ----------------\n",
        "def hours_from_manifest(profile: str):\n",
        "    try:\n",
        "        m = pd.read_csv(MANIFEST)\n",
        "        m = m[m[\"profile\"]==profile]\n",
        "        return float(pd.to_numeric(m[\"duration\"], errors=\"coerce\").fillna(0).sum()/3600.0), len(m)\n",
        "    except Exception:\n",
        "        return 0.0, 0\n",
        "\n",
        "outs = list((OUT_ROOT/PROFILE).rglob(\"*.wav\"))\n",
        "print(f\"[{PROFILE}] outputs on disk: {len(outs):,} (showing a few)\")\n",
        "for p in outs[:8]:\n",
        "    print(\"  \", p)\n",
        "\n",
        "hrs, nrows = hours_from_manifest(PROFILE)\n",
        "print(f\"[{PROFILE}] manifest rows: {nrows:,} | hours: {hrs:.2f} h\")\n",
        "\n",
        "# keep manifest dedupbed (safety)\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST)\n",
        "    before = len(mf)\n",
        "    mf = mf.drop_duplicates(subset=[\"profile\",\"path_out\"], keep=\"first\").reset_index(drop=True)\n",
        "    if len(mf) != before:\n",
        "        mf.to_csv(MANIFEST, index=False)\n",
        "        print(f\"[fix] dedup manifest: -{before-len(mf)} rows\")\n",
        "print(\"[OK] BASE preprocessing done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Test Set Integration (Fake) - Strong Profile**\n",
        "We apply the full augmentation pipeline (RawBoost + EQ) to the newly added **fake test set** files, ensuring they are consistent with the rest of the `strong` dataset."
      ],
      "metadata": {
        "id": "enIlG_UVL5wV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "40b7684e8cbf4c4fa2abdf05d886ef51",
            "a424c89e9b9b4c818582275501b6a395",
            "6c559db3b41745c8953ac108f10f014f",
            "459bdd2431a54d5aac9c2d66656c7395",
            "afe6a91486f840a8b3f202beeb47a86f",
            "982f9de5500a4d0a8439e03ed8c1ff56",
            "08c0a8dc250b4c13a556eb23dc90c850",
            "bc7a0185a6cf4e9aaa3421454b8b4fb3",
            "13dbf09fabc5494b8aff80601d10b687",
            "9951761addc446a8b6d82c48632fea3d",
            "11f9b40ce3114b86b2c81e7accade8f7"
          ]
        },
        "id": "zFDlKoqOyuq9",
        "outputId": "75e14711-6164-4b1b-c815-0e9fae98abd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input counts (normalized):\n",
            "label\n",
            "fake    48170\n",
            "real    49623\n",
            "\n",
            "[preproc:strong] PLANNED=97,793 | ALREADY_DONE=94,789 | TO_DO=3,004\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40b7684e8cbf4c4fa2abdf05d886ef51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preproc:strong:   0%|          | 0/3004 [00:00<?, ?file/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[strong] FINISHED: +3004 files, 0 errors | elapsed 10.1 min\n",
            "[strong] Manifest rows: 97,793 | hours: 185.21 h\n",
            "[OK] STRONG preprocessing done.\n"
          ]
        }
      ],
      "source": [
        "# ================= PREPROCESSING — STRONG (EQ + RawBoost v3) =================\n",
        "# Writes: /content/drive/MyDrive/hindi_dfake/processed/wav/strong/...\n",
        "# Logs to: /content/drive/MyDrive/hindi_dfake/metadata/proc_manifest.csv (profile=strong)\n",
        "\n",
        "import os, csv, uuid, time, hashlib, subprocess\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np, soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ------------ CONFIG ------------\n",
        "ROOT = Path(\"/content/drive/MyDrive/hindi_dfake\")\n",
        "META = ROOT / \"metadata\"\n",
        "OUT_ROOT = ROOT / \"processed\" / \"wav\"\n",
        "\n",
        "PROFILE = \"strong\"          # strong = base + artifact EQ + RawBoost v3\n",
        "INCLUDE_ATTACKS = True\n",
        "FRACTION_REAL = 1.00        # process all\n",
        "FRACTION_FAKE = 1.00\n",
        "HASH_SEED = 2025\n",
        "\n",
        "# Loudness & trim\n",
        "TARGET_DB   = -26.0\n",
        "TRIM_THR_DB = -45\n",
        "TRIM_DUR_S  = 0.20\n",
        "\n",
        "# Concurrency / batching (safe defaults for Colab)\n",
        "MAX_WORKERS   = 6\n",
        "INFLIGHT_MULT = 3\n",
        "BATCH_WRITE   = 500\n",
        "\n",
        "# RawBoost v3–style knobs (same as we used earlier)\n",
        "RB_ENABLE           = True\n",
        "RB_BANDPASS_HZ      = (2500, 6000)\n",
        "RB_SNR_DB           = 25.0\n",
        "RB_NOISE_PROB       = 1.0\n",
        "RB_IMPULSE_PROB     = 0.5\n",
        "RB_IMPULSES_PER_SEC = 1.0\n",
        "RB_IMPULSE_GAIN     = 0.08\n",
        "RB_REVERB_PROB      = 0.6\n",
        "RB_REVERB_T_SEC     = 0.03\n",
        "RB_REVERB_DECAY     = 0.35\n",
        "RB_REVERB_WET       = 0.18\n",
        "\n",
        "REAL_CSV  = META / \"master_real.csv\"\n",
        "FAKE_CSV  = META / \"master_fake.csv\"\n",
        "ATT_CSV   = META / \"attacks.labeled.csv\"\n",
        "MANIFEST  = META / \"proc_manifest.csv\"\n",
        "assert REAL_CSV.exists() and FAKE_CSV.exists()\n",
        "\n",
        "# ------------ helpers ------------\n",
        "def read_csv_safe(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    for c in [\"utt_id\",\"path\",\"duration\",\"label\",\"fake_type\",\"source\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\" if c != \"duration\" else 0.0\n",
        "    return df\n",
        "\n",
        "def normalize_label(lab, fake_type, src, path):\n",
        "    l = str(lab).strip().lower()\n",
        "    if l in {\"real\",\"fake\"}: return l\n",
        "    ft = str(fake_type).strip().lower()\n",
        "    s  = str(src).strip().lower()\n",
        "    p  = str(path)\n",
        "    if \"/raw/real_clean\" in p or s.startswith(\"real_\"): return \"real\"\n",
        "    if ft in {\"tts_edge\",\"channel_attack\",\"vc\"} or \"/raw/fake_\" in p or s.startswith(\"fake_\"): return \"fake\"\n",
        "    return \"fake\"\n",
        "\n",
        "def stable_frac_mask(series: pd.Series, frac: float, seed: int) -> pd.Series:\n",
        "    if frac >= 1.0: return pd.Series([True]*len(series), index=series.index)\n",
        "    s = series.astype(str).apply(lambda p: int(hashlib.sha1((p+str(seed)).encode()).hexdigest()[:8], 16) / 0xFFFFFFFF)\n",
        "    return s < frac\n",
        "\n",
        "def out_path_for(in_path: str) -> Path:\n",
        "    p = Path(in_path)\n",
        "    try:    rel = p.relative_to(ROOT)\n",
        "    except: rel = Path(\"_external\") / p.name\n",
        "    return OUT_ROOT / PROFILE / rel\n",
        "\n",
        "def ensure_parent(p: Path): p.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def rms_dbfs_arr(x: np.ndarray) -> float:\n",
        "    if x.ndim > 1: x = x.mean(axis=1)\n",
        "    if len(x) == 0: return -120.0\n",
        "    rms = float(np.sqrt(np.mean(np.square(x))))\n",
        "    if rms <= 1e-9: return -120.0\n",
        "    return 20.0*np.log10(np.clip(rms, 1e-9, 1.0))\n",
        "\n",
        "# ---- RawBoost v3–style (deterministic per file) ----\n",
        "def _rng_from_key(key: str):\n",
        "    seed = int(hashlib.sha1(key.encode(\"utf-8\")).hexdigest()[:8], 16)\n",
        "    return np.random.default_rng(seed)\n",
        "\n",
        "def _add_colored_noise(x, sr, rng, band=(2500,6000), snr_db=25.0):\n",
        "    n = len(x)\n",
        "    if n == 0: return x\n",
        "    wn = rng.standard_normal(n).astype(np.float32)\n",
        "    X = np.fft.rfft(wn); freqs = np.fft.rfftfreq(n, d=1.0/sr)\n",
        "    X[~((freqs >= band[0]) & (freqs <= band[1]))] = 0.0\n",
        "    noise = np.fft.irfft(X, n=n).astype(np.float32)\n",
        "    sig_rms = np.sqrt(np.mean(np.square(x))) + 1e-9\n",
        "    target_noise_rms = sig_rms / (10.0**(snr_db/20.0))\n",
        "    cur_rms = np.sqrt(np.mean(np.square(noise))) + 1e-12\n",
        "    noise *= (target_noise_rms / cur_rms)\n",
        "    return np.clip(x + noise, -1.0, 1.0)\n",
        "\n",
        "def _add_impulses(x, sr, rng, per_sec=1.0, gain=0.08):\n",
        "    n = len(x)\n",
        "    if n == 0: return x\n",
        "    dur = n/float(sr); k = max(1, int(per_sec * dur))\n",
        "    idx = rng.integers(0, n, size=k)\n",
        "    amp = gain * (np.sqrt(np.mean(np.square(x))) + 1e-9)\n",
        "    y = x.copy()\n",
        "    y[idx] = np.clip(y[idx] + amp * rng.choice([-1.0, 1.0], size=k), -1.0, 1.0)\n",
        "    return y\n",
        "\n",
        "def _add_small_reverb(x, sr, rng, t_sec=0.03, decay=0.35, wet=0.18):\n",
        "    n = len(x)\n",
        "    if n == 0: return x\n",
        "    ir_len = max(8, int(t_sec * sr))\n",
        "    t = np.arange(ir_len, dtype=np.float32)\n",
        "    ir = np.exp(-decay * t / ir_len).astype(np.float32)\n",
        "    for _ in range(3):\n",
        "        pos = int(rng.integers(0, ir_len))\n",
        "        ir[pos] += float(rng.uniform(0.1, 0.3))\n",
        "    ir /= (np.sum(np.abs(ir)) + 1e-9)\n",
        "    y = np.convolve(x, ir, mode=\"full\")[:n].astype(np.float32)\n",
        "    return np.clip((1.0 - wet) * x + wet * y, -1.0, 1.0)\n",
        "\n",
        "def rawboost_v3(x: np.ndarray, sr: int, key: str) -> np.ndarray:\n",
        "    if x.ndim > 1: x = x.mean(axis=1)\n",
        "    x = np.clip(x, -1.0, 1.0).astype(np.float32)\n",
        "    if not RB_ENABLE: return x\n",
        "    rng = _rng_from_key(key)\n",
        "    if rng.uniform() < RB_NOISE_PROB:\n",
        "        x = _add_colored_noise(x, sr, rng, RB_BANDPASS_HZ, RB_SNR_DB)\n",
        "    if rng.uniform() < RB_IMPULSE_PROB:\n",
        "        x = _add_impulses(x, sr, rng, RB_IMPULSES_PER_SEC, RB_IMPULSE_GAIN)\n",
        "    if rng.uniform() < RB_REVERB_PROB:\n",
        "        x = _add_small_reverb(x, sr, rng, RB_REVERB_T_SEC, RB_REVERB_DECAY, RB_REVERB_WET)\n",
        "    return x\n",
        "\n",
        "def build_filter_chain_strong(gain_db: float) -> str:\n",
        "    return \",\".join([\n",
        "        \"highpass=f=20\",\n",
        "        \"equalizer=f=3000:t=q:w=1.0:g=2.5\",\n",
        "        \"equalizer=f=4800:t=q:w=0.9:g=2.0\",\n",
        "        \"treble=g=1.0:f=6000:t=h:w=0.7\",\n",
        "        f\"volume={gain_db}dB\",\n",
        "        f\"silenceremove=start_periods=1:start_duration={TRIM_DUR_S}:start_threshold={TRIM_THR_DB}dB:stop_periods=1:stop_duration={TRIM_DUR_S}:stop_threshold={TRIM_THR_DB}dB\"\n",
        "    ])\n",
        "\n",
        "# ------------ load inputs ------------\n",
        "dfR = read_csv_safe(REAL_CSV)\n",
        "dfF = read_csv_safe(FAKE_CSV)\n",
        "dfs = [dfR, dfF]\n",
        "if INCLUDE_ATTACKS and ATT_CSV.exists():\n",
        "    dfs.append(read_csv_safe(ATT_CSV))\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "df_all = df_all[df_all[\"path\"].astype(str).apply(lambda p: Path(p).exists())].copy()\n",
        "df_all[\"label\"] = [normalize_label(l, ft, s, p) for l, ft, s, p in\n",
        "                   zip(df_all[\"label\"], df_all[\"fake_type\"], df_all[\"source\"], df_all[\"path\"])]\n",
        "\n",
        "# deterministic per-label subset (here: everything, but we still keep the hook)\n",
        "parts = []\n",
        "for lab, g in df_all.groupby(df_all[\"label\"].astype(str).str.lower()):\n",
        "    frac = FRACTION_REAL if lab == \"real\" else FRACTION_FAKE\n",
        "    parts.append(g if frac>=1.0 else g[stable_frac_mask(g[\"path\"], frac, HASH_SEED)])\n",
        "df_in = pd.concat(parts, ignore_index=True) if parts else df_all.copy()\n",
        "\n",
        "# ------------ manifest skip logic ------------\n",
        "MANIFEST.parent.mkdir(parents=True, exist_ok=True)\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST, usecols=[\"profile\",\"path_out\"])\n",
        "    done_set = set(mf.loc[mf[\"profile\"]==PROFILE, \"path_out\"].astype(str))\n",
        "else:\n",
        "    with open(MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "    done_set = set()\n",
        "\n",
        "def already_done(pin: str) -> bool:\n",
        "    pout = out_path_for(pin)\n",
        "    return pout.exists() or (str(pout) in done_set)\n",
        "\n",
        "# Only process files that don't have a STRONG output yet\n",
        "todo_rows = [r for _, r in df_in.iterrows() if not already_done(str(r[\"path\"]))]\n",
        "\n",
        "planned, todo = len(df_in), len(todo_rows)\n",
        "print(\"Input counts (normalized):\")\n",
        "print(df_all.groupby(df_all['label']).size().to_string())\n",
        "print(f\"\\n[preproc:{PROFILE}] PLANNED={planned:,} | ALREADY_DONE={planned-todo:,} | TO_DO={todo:,}\")\n",
        "\n",
        "# ------------ worker ------------\n",
        "def preprocess_one(row):\n",
        "    pin = str(row[\"path\"])\n",
        "    pout = out_path_for(pin)\n",
        "    ensure_parent(pout)\n",
        "\n",
        "    # read (and resample if needed)\n",
        "    try:\n",
        "        x, sr = sf.read(pin, dtype=\"float32\", always_2d=False)\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"read_fail: {e}\", \"path_in\": pin}\n",
        "    if x.ndim > 1: x = x.mean(axis=1)\n",
        "    if sr != 16000:\n",
        "        tmp = pout.parent / f\".{pout.stem}.in-{uuid.uuid4().hex}.wav\"\n",
        "        cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "               \"-i\", pin, \"-ac\",\"1\",\"-ar\",\"16000\",\"-sample_fmt\",\"s16\", str(tmp)]\n",
        "        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        if p.returncode != 0:\n",
        "            try: tmp.unlink(missing_ok=True)\n",
        "            except: pass\n",
        "            return {\"error\": \"resample_fail\", \"path_in\": pin}\n",
        "        x, sr = sf.read(str(tmp), dtype=\"float32\", always_2d=False)\n",
        "        try: tmp.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "\n",
        "    x = np.clip(x, -1.0, 1.0).astype(np.float32)\n",
        "\n",
        "    # RawBoost v3–style\n",
        "    key = f\"{pin}|{PROFILE}\"\n",
        "    try:    y = rawboost_v3(x, sr, key)\n",
        "    except: y = x\n",
        "\n",
        "    # write RB temp near target (avoid cross-device link)\n",
        "    tmp_rb = pout.parent / f\".{pout.stem}.rb-{uuid.uuid4().hex}.wav\"\n",
        "    sf.write(str(tmp_rb), y, 16000, subtype=\"PCM_16\")\n",
        "\n",
        "    # EQ + loudness + trim via ffmpeg\n",
        "    gain_db = float(np.clip(TARGET_DB - rms_dbfs_arr(y), -20.0, 20.0))\n",
        "    FILTER  = build_filter_chain_strong(gain_db)\n",
        "\n",
        "    tmp_out = pout.parent / f\".{pout.stem}.tmp-{uuid.uuid4().hex}.wav\"\n",
        "    cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "           \"-i\", str(tmp_rb), \"-ac\",\"1\",\"-ar\",\"16000\",\n",
        "           \"-af\", FILTER, \"-sample_fmt\",\"s16\", str(tmp_out)]\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    try: tmp_rb.unlink(missing_ok=True)\n",
        "    except: pass\n",
        "    if p.returncode != 0:\n",
        "        try: tmp_out.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "        return {\"error\": p.stderr.decode(\"utf-8\",\"ignore\")[:300], \"path_in\": pin}\n",
        "\n",
        "    os.replace(str(tmp_out), str(pout))\n",
        "\n",
        "    # stats\n",
        "    try:\n",
        "        z, sr2 = sf.read(str(pout), dtype=\"float32\", always_2d=False)\n",
        "        if z.ndim > 1: z = z.mean(axis=1)\n",
        "        dur = len(z)/float(sr2) if sr2 else 0.0\n",
        "        peak = float(np.max(np.abs(z))) if len(z) else 0.0\n",
        "        rms  = float(np.sqrt(np.mean(np.square(z)))) if len(z) else 0.0\n",
        "        rms_db = -120.0 if rms<=1e-9 else 20*np.log10(np.clip(rms,1e-9,1.0))\n",
        "        peak_db = -120.0 if peak<=1e-9 else 20*np.log10(np.clip(peak,1e-9,1.0))\n",
        "    except Exception:\n",
        "        dur, rms_db, peak_db = 0.0, -120.0, -120.0\n",
        "\n",
        "    return {\"utt_id\": row.get(\"utt_id\",\"\"), \"path_in\": pin, \"path_out\": str(pout),\n",
        "            \"profile\": PROFILE, \"duration\": round(dur,3),\n",
        "            \"rms_db\": round(rms_db,2), \"peak_db\": round(peak_db,2)}\n",
        "\n",
        "# ------------ streaming scheduler with progress ------------\n",
        "created = 0; errs = 0; buf = []\n",
        "start = time.time()\n",
        "inflight_cap = max(MAX_WORKERS * INFLIGHT_MULT, MAX_WORKERS)\n",
        "pbar = tqdm(total=len(todo_rows), desc=f\"preproc:{PROFILE}\", unit=\"file\")\n",
        "\n",
        "def flush_manifest(rows):\n",
        "    if not rows: return\n",
        "    with open(MANIFEST, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
        "        w = csv.DictWriter(fcsv, fieldnames=[\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "        for r in rows: w.writerow(r)\n",
        "\n",
        "it = iter(todo_rows)\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    inflight = set()\n",
        "    for _ in range(min(inflight_cap, len(todo_rows))):\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: break\n",
        "    while inflight:\n",
        "        done = next(as_completed(inflight))\n",
        "        inflight.remove(done)\n",
        "        res = done.result()\n",
        "        pbar.update(1)\n",
        "        if isinstance(res, dict) and \"error\" in res:\n",
        "            errs += 1\n",
        "        elif res:\n",
        "            buf.append(res); created += 1\n",
        "            if len(buf) >= BATCH_WRITE:\n",
        "                flush_manifest(buf); buf.clear()\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: pass\n",
        "\n",
        "flush_manifest(buf)\n",
        "pbar.close()\n",
        "print(f\"\\n[{PROFILE}] FINISHED: +{created} files, {errs} errors | elapsed {(time.time()-start)/60:.1f} min\")\n",
        "\n",
        "# quick manifest slice\n",
        "if MANIFEST.exists():\n",
        "    m = pd.read_csv(MANIFEST)\n",
        "    mp = m[m[\"profile\"]==PROFILE]\n",
        "    hrs = float(pd.to_numeric(mp[\"duration\"], errors=\"coerce\").fillna(0).sum()/3600.0)\n",
        "    print(f\"[{PROFILE}] Manifest rows: {len(mp):,} | hours: {hrs:.2f} h\")\n",
        "\n",
        "print(\"[OK] STRONG preprocessing done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9. Test Set Integration (Real) - Base Profile**\n",
        "We process the **FLEURS** dataset (Real Test Set), normalizing the 604 files to match the `base` profile (16kHz, mono, -26dB) for consistent evaluation."
      ],
      "metadata": {
        "id": "Sn9CWG9GMOOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== PREPROCESS FLEURS (BASE profile; exact same chain as MMS-TTS) ==========\n",
        "# Pipeline: DC high-pass -> per-file gain to TARGET_DB -> light trim\n",
        "# Writes: /content/drive/MyDrive/hindi_dfake/processed/wav/base/...\n",
        "# Logs  : /content/drive/MyDrive/hindi_dfake/metadata/proc_manifest.csv (profile=base)\n",
        "\n",
        "import os, csv, time, uuid, hashlib, subprocess\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np, soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "ROOT = Path(\"/content/drive/MyDrive/hindi_dfake\")\n",
        "META = ROOT / \"metadata\"\n",
        "OUT_ROOT = ROOT / \"processed\" / \"wav\"\n",
        "\n",
        "PROFILE = \"base\"          # <- BASE profile (unchanged)\n",
        "HASH_SEED = 2025\n",
        "\n",
        "# Loudness & trim (identical to your MMS-TTS run)\n",
        "TARGET_DB   = -26.0\n",
        "TRIM_THR_DB = -45\n",
        "TRIM_DUR_S  = 0.20\n",
        "\n",
        "# Concurrency / batching\n",
        "MAX_WORKERS   = 6\n",
        "INFLIGHT_MULT = 3\n",
        "BATCH_WRITE   = 500\n",
        "\n",
        "FLEURS_CSV = META / \"thirdparty_real_test.fleurs.csv\"   # <-- input list (FLEURS only)\n",
        "MANIFEST   = META / \"proc_manifest.csv\"\n",
        "\n",
        "assert FLEURS_CSV.exists(), f\"Missing {FLEURS_CSV}\"\n",
        "\n",
        "# ---------------- helpers (unchanged) ----------------\n",
        "def read_csv_safe(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    for c in [\"utt_id\",\"path\",\"duration\",\"label\",\"fake_type\",\"source\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\" if c != \"duration\" else 0.0\n",
        "    return df\n",
        "\n",
        "def normalize_label(lab, fake_type, src, path):\n",
        "    l = str(lab).strip().lower()\n",
        "    if l in {\"real\",\"fake\"}: return l\n",
        "    ft = str(fake_type).strip().lower()\n",
        "    s  = str(src).strip().lower()\n",
        "    p  = str(path)\n",
        "    if \"/raw/real_clean\" in p or s.startswith(\"real_\"): return \"real\"\n",
        "    if ft in {\"tts_edge\",\"channel_attack\",\"vc\"} or \"/raw/fake_\" in p or s.startswith(\"fake_\"): return \"fake\"\n",
        "    return \"fake\"\n",
        "\n",
        "def out_path_for(in_path: str) -> Path:\n",
        "    p = Path(in_path)\n",
        "    try:    rel = p.relative_to(ROOT)\n",
        "    except: rel = Path(\"_external\") / p.name\n",
        "    return OUT_ROOT / PROFILE / rel\n",
        "\n",
        "def ensure_parent(p: Path): p.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def rms_dbfs(p: Path) -> float:\n",
        "    try:\n",
        "        x, sr = sf.read(str(p), dtype=\"float32\", always_2d=False)\n",
        "        if hasattr(x, \"ndim\") and x.ndim > 1: x = x.mean(axis=1)\n",
        "        if len(x) == 0: return -120.0\n",
        "        rms = float(np.sqrt(np.mean(np.square(x))))\n",
        "        if rms <= 1e-9: return -120.0\n",
        "        return 20.0*np.log10(np.clip(rms, 1e-9, 1.0))\n",
        "    except Exception:\n",
        "        return -120.0\n",
        "\n",
        "def build_filter_chain_base(gain_db: float) -> str:\n",
        "    return \",\".join([\n",
        "        \"highpass=f=20\",\n",
        "        f\"volume={gain_db}dB\",\n",
        "        f\"silenceremove=start_periods=1:start_duration={TRIM_DUR_S}:start_threshold={TRIM_THR_DB}dB:\"\n",
        "        f\"stop_periods=1:stop_duration={TRIM_DUR_S}:stop_threshold={TRIM_THR_DB}dB\"\n",
        "    ])\n",
        "\n",
        "# ---------------- load inputs (FLEURS ONLY) ----------------\n",
        "df_fl = read_csv_safe(FLEURS_CSV)\n",
        "# keep only files that exist on disk\n",
        "df_fl = df_fl[df_fl[\"path\"].astype(str).apply(lambda p: Path(p).exists())].copy()\n",
        "# force label to real (FLEURS is real speech)\n",
        "df_fl[\"label\"] = [normalize_label(\"real\", ft, s, p) for ft, s, p in\n",
        "                  zip(df_fl[\"fake_type\"], df_fl[\"source\"], df_fl[\"path\"])]\n",
        "\n",
        "print(\"Input counts (FLEURS only, normalized):\")\n",
        "print(df_fl.groupby(df_fl[\"label\"].astype(str).str.lower()).size().to_string())\n",
        "\n",
        "df_in = df_fl.copy()  # all fleurs; resume logic below will skip already-done\n",
        "\n",
        "# ---------------- manifest / resume ----------------\n",
        "MANIFEST.parent.mkdir(parents=True, exist_ok=True)\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST, usecols=[\"profile\",\"path_out\"])\n",
        "    done_set = set(mf.loc[mf[\"profile\"]==PROFILE, \"path_out\"].astype(str))\n",
        "else:\n",
        "    with open(MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "    done_set = set()\n",
        "\n",
        "def already_done(pin: str) -> bool:\n",
        "    pout = out_path_for(pin)\n",
        "    return pout.exists() or (str(pout) in done_set)\n",
        "\n",
        "todo_rows = [r for _, r in df_in.iterrows() if not already_done(str(r[\"path\"]))]\n",
        "planned, todo = len(df_in), len(todo_rows)\n",
        "print(f\"\\n[preproc:{PROFILE} | FLEURS] PLANNED={planned:,} | ALREADY_DONE={planned-todo:,} | TO_DO={todo:,}\")\n",
        "\n",
        "# ---------------- worker (BASE chain; identical) ----------------\n",
        "def preprocess_one(row):\n",
        "    pin = str(row[\"path\"])\n",
        "    pout = out_path_for(pin)\n",
        "    ensure_parent(pout)\n",
        "\n",
        "    # If input SR != 16k or multichannel, resample first\n",
        "    try:\n",
        "        x, sr = sf.read(pin, dtype=\"float32\", always_2d=False)\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"read_fail: {e}\", \"path_in\": pin}\n",
        "    if (sr != 16000) or (hasattr(x, \"ndim\") and x.ndim > 1):\n",
        "        tmp_in = pout.parent / f\".{pout.stem}.in-{uuid.uuid4().hex}.wav\"\n",
        "        cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "               \"-i\", pin, \"-ac\",\"1\",\"-ar\",\"16000\",\"-sample_fmt\",\"s16\", str(tmp_in)]\n",
        "        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        if p.returncode != 0:\n",
        "            try: tmp_in.unlink(missing_ok=True)\n",
        "            except: pass\n",
        "            return {\"error\": \"resample_fail\", \"path_in\": pin}\n",
        "        pin_for_chain = str(tmp_in)\n",
        "    else:\n",
        "        pin_for_chain = pin\n",
        "\n",
        "    gain_db = float(np.clip(TARGET_DB - rms_dbfs(Path(pin_for_chain)), -20.0, 20.0))\n",
        "    FILTER  = build_filter_chain_base(gain_db)\n",
        "\n",
        "    tmp_out = pout.parent / f\".{pout.stem}.tmp-{uuid.uuid4().hex}.wav\"\n",
        "    cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "           \"-i\", pin_for_chain, \"-ac\",\"1\",\"-ar\",\"16000\",\n",
        "           \"-af\", FILTER, \"-sample_fmt\",\"s16\", str(tmp_out)]\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    # clean temp (if created)\n",
        "    if pin_for_chain != pin:\n",
        "        try: Path(pin_for_chain).unlink(missing_ok=True)\n",
        "        except: pass\n",
        "\n",
        "    if p.returncode != 0:\n",
        "        try: tmp_out.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "        return {\"error\": p.stderr.decode(\"utf-8\",\"ignore\")[:300], \"path_in\": pin}\n",
        "\n",
        "    os.replace(str(tmp_out), str(pout))\n",
        "\n",
        "    # quick stats\n",
        "    try:\n",
        "        z, sr2 = sf.read(str(pout), dtype=\"float32\", always_2d=False)\n",
        "        if hasattr(z, \"ndim\") and z.ndim > 1: z = z.mean(axis=1)\n",
        "        dur = len(z)/float(sr2) if sr2 else 0.0\n",
        "        peak = float(np.max(np.abs(z))) if len(z) else 0.0\n",
        "        rms  = float(np.sqrt(np.mean(np.square(z)))) if len(z) else 0.0\n",
        "        rms_db = -120.0 if rms<=1e-9 else 20*np.log10(np.clip(rms,1e-9,1.0))\n",
        "        peak_db = -120.0 if peak<=1e-9 else 20*np.log10(np.clip(peak,1e-9,1.0))\n",
        "    except Exception:\n",
        "        dur, rms_db, peak_db = 0.0, -120.0, -120.0\n",
        "\n",
        "    return {\"utt_id\": row.get(\"utt_id\",\"\"), \"path_in\": pin, \"path_out\": str(pout),\n",
        "            \"profile\": PROFILE, \"duration\": round(dur,3),\n",
        "            \"rms_db\": round(rms_db,2), \"peak_db\": round(peak_db,2)}\n",
        "\n",
        "# ---------------- scheduler + progress (unchanged) ----------------\n",
        "created = 0; errs = 0; buf = []\n",
        "start = time.time()\n",
        "inflight_cap = max(MAX_WORKERS * INFLIGHT_MULT, MAX_WORKERS)\n",
        "pbar = tqdm(total=len(todo_rows), desc=f\"preproc:{PROFILE}:FLEURS\", unit=\"file\")\n",
        "\n",
        "def flush_manifest(rows):\n",
        "    if not rows: return\n",
        "    with open(MANIFEST, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
        "        w = csv.DictWriter(fcsv, fieldnames=[\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "        for r in rows: w.writerow(r)\n",
        "\n",
        "it = iter(todo_rows)\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    inflight = set()\n",
        "    for _ in range(min(inflight_cap, len(todo_rows))):\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: break\n",
        "    while inflight:\n",
        "        done = next(as_completed(inflight))\n",
        "        inflight.remove(done)\n",
        "        res = done.result()\n",
        "        pbar.update(1)\n",
        "        if isinstance(res, dict) and \"error\" in res:\n",
        "            errs += 1\n",
        "        elif res:\n",
        "            buf.append(res); created += 1\n",
        "            if len(buf) >= BATCH_WRITE:\n",
        "                flush_manifest(buf); buf.clear()\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: pass\n",
        "\n",
        "flush_manifest(buf)\n",
        "pbar.close()\n",
        "print(f\"\\n[{PROFILE}:FLEURS] FINISHED: +{created} files, {errs} errors | elapsed {(time.time()-start)/60:.1f} min\")\n",
        "\n",
        "# ---------------- quick audit / summary (unchanged) ----------------\n",
        "def hours_from_manifest(profile: str):\n",
        "    try:\n",
        "        m = pd.read_csv(MANIFEST)\n",
        "        m = m[m[\"profile\"]==profile]\n",
        "        return float(pd.to_numeric(m[\"duration\"], errors=\"coerce\").fillna(0).sum()/3600.0), len(m)\n",
        "    except Exception:\n",
        "        return 0.0, 0\n",
        "\n",
        "outs = list((OUT_ROOT/PROFILE).rglob(\"*.wav\"))\n",
        "print(f\"[{PROFILE}] outputs on disk: {len(outs):,} (showing a few)\")\n",
        "for p in outs[:8]:\n",
        "    print(\"  \", p)\n",
        "\n",
        "hrs, nrows = hours_from_manifest(PROFILE)\n",
        "print(f\"[{PROFILE}] manifest rows: {nrows:,} | hours: {hrs:.2f} h\")\n",
        "\n",
        "# keep manifest dedupbed (safety)\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST)\n",
        "    before = len(mf)\n",
        "    mf = mf.drop_duplicates(subset=[\"profile\",\"path_out\"], keep=\"first\").reset_index(drop=True)\n",
        "    if len(mf) != before:\n",
        "        mf.to_csv(MANIFEST, index=False)\n",
        "        print(f\"[fix] dedup manifest: -{before-len(mf)} rows\")\n",
        "print(\"[OK] BASE preprocessing for FLEURS done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361,
          "referenced_widgets": [
            "24b50b563df84cd9863060f07958c862",
            "326dd133af29439baa03a8840e13ea28",
            "c5224ebedb0240b3b0d2cd7686ee2a90",
            "9fc8ee28864b4c97a8d8e08019c9c0b3",
            "8d69a7783f0c44dbb2735787d57ef2aa",
            "32f854f2b65b4a0c812c51f544cbaa9f",
            "49feaaf30e784d16811af68d8438ad35",
            "40fcd6481c1046b9bcf26cb75740f917",
            "0402b900415e4fd2850e55250ae685a8",
            "2e3327c81b8d48e8bee5086876bab24e",
            "fc954db8d44e458ea3e865aaf99e98aa"
          ]
        },
        "id": "2yOA_AwT_QPq",
        "outputId": "86814608-e4af-4bc2-ed12-48fd113c187a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input counts (FLEURS only, normalized):\n",
            "label\n",
            "real    604\n",
            "\n",
            "[preproc:base | FLEURS] PLANNED=604 | ALREADY_DONE=0 | TO_DO=604\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preproc:base:FLEURS:   0%|          | 0/604 [00:00<?, ?file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24b50b563df84cd9863060f07958c862"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[base:FLEURS] FINISHED: +604 files, 0 errors | elapsed 2.2 min\n",
            "[base] outputs on disk: 98,397 (showing a few)\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_6859d4f039aea68f.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_19e1c8d7cad439c0.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_a5ea2af5277b57c4.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_7aee2f81107fe9de.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_c103d799a208b3c8.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_8883f29f2fb7bfdf.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_e43afa0cec72f0aa.wav\n",
            "   /content/drive/MyDrive/hindi_dfake/processed/wav/base/raw/fake_tts/tts_edge_47ea9317998de3aa.wav\n",
            "[base] manifest rows: 98,397 | hours: 79.59 h\n",
            "[OK] BASE preprocessing for FLEURS done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10. Test Set Integration (Real) - Strong Profile**\n",
        "We apply the full `strong` augmentation pipeline (RawBoost + EQ) to the **FLEURS** dataset (Real Test Set). This ensures we can evaluate the model's performance on \"hard,\" noisy real speech as well as clean speech.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YZrGv0YUMkPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= PREPROCESS FLEURS — STRONG (EQ + RawBoost v3) =================\n",
        "# Writes: /content/drive/MyDrive/hindi_dfake/processed/wav/strong/...\n",
        "# Logs :  /content/drive/MyDrive/hindi_dfake/metadata/proc_manifest.csv (profile=strong)\n",
        "# Scope:  ONLY rows from metadata/thirdparty_real_test.fleurs.csv (resume-safe)\n",
        "\n",
        "import os, csv, uuid, time, hashlib, subprocess\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np, soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ------------ CONFIG ------------\n",
        "ROOT = Path(\"/content/drive/MyDrive/hindi_dfake\")\n",
        "META = ROOT / \"metadata\"\n",
        "OUT_ROOT = ROOT / \"processed\" / \"wav\"\n",
        "\n",
        "PROFILE = \"strong\"          # strong = base + artifact EQ + RawBoost v3 (same as MMS-TTS)\n",
        "HASH_SEED = 2025\n",
        "\n",
        "# Loudness & trim (identical to your MMS-TTS STRONG run)\n",
        "TARGET_DB   = -26.0\n",
        "TRIM_THR_DB = -45\n",
        "TRIM_DUR_S  = 0.20\n",
        "\n",
        "# Concurrency / batching\n",
        "MAX_WORKERS   = 6\n",
        "INFLIGHT_MULT = 3\n",
        "BATCH_WRITE   = 500\n",
        "\n",
        "FLEURS_CSV = META / \"thirdparty_real_test.fleurs.csv\"   # <-- input list (FLEURS only)\n",
        "MANIFEST   = META / \"proc_manifest.csv\"\n",
        "assert FLEURS_CSV.exists(), f\"Missing {FLEURS_CSV}\"\n",
        "\n",
        "# ------------ helpers (unchanged logic from your STRONG job) ------------\n",
        "def read_csv_safe(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    for c in [\"utt_id\",\"path\",\"duration\",\"label\",\"fake_type\",\"source\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\" if c != \"duration\" else 0.0\n",
        "    return df\n",
        "\n",
        "def out_path_for(in_path: str) -> Path:\n",
        "    p = Path(in_path)\n",
        "    try:    rel = p.relative_to(ROOT)\n",
        "    except: rel = Path(\"_external\") / p.name\n",
        "    return OUT_ROOT / PROFILE / rel\n",
        "\n",
        "def ensure_parent(p: Path): p.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def rms_dbfs_arr(x: np.ndarray) -> float:\n",
        "    if x.ndim > 1: x = x.mean(axis=1)\n",
        "    if len(x) == 0: return -120.0\n",
        "    rms = float(np.sqrt(np.mean(np.square(x))))\n",
        "    if rms <= 1e-9: return -120.0\n",
        "    return 20.0*np.log10(np.clip(rms, 1.0e-9, 1.0))\n",
        "\n",
        "# ---- RawBoost v3–style (deterministic per file) ----\n",
        "def _rng_from_key(key: str):\n",
        "    seed = int(hashlib.sha1(key.encode(\"utf-8\")).hexdigest()[:8], 16)\n",
        "    return np.random.default_rng(seed)\n",
        "\n",
        "def _add_colored_noise(x, sr, rng, band=(2500,6000), snr_db=25.0):\n",
        "    n = len(x)\n",
        "    if n == 0: return x\n",
        "    wn = rng.standard_normal(n).astype(np.float32)\n",
        "    X = np.fft.rfft(wn); freqs = np.fft.rfftfreq(n, d=1.0/sr)\n",
        "    X[~((freqs >= band[0]) & (freqs <= band[1]))] = 0.0\n",
        "    noise = np.fft.irfft(X, n=n).astype(np.float32)\n",
        "    sig_rms = np.sqrt(np.mean(np.square(x))) + 1e-9\n",
        "    target_noise_rms = sig_rms / (10.0**(snr_db/20.0))\n",
        "    cur_rms = np.sqrt(np.mean(np.square(noise))) + 1e-12\n",
        "    noise *= (target_noise_rms / cur_rms)\n",
        "    return np.clip(x + noise, -1.0, 1.0)\n",
        "\n",
        "def _add_impulses(x, sr, rng, per_sec=1.0, gain=0.08):\n",
        "    n = len(x)\n",
        "    if n == 0: return x\n",
        "    dur = n/float(sr); k = max(1, int(per_sec * dur))\n",
        "    idx = rng.integers(0, n, size=k)\n",
        "    amp = gain * (np.sqrt(np.mean(np.square(x))) + 1e-9)\n",
        "    y = x.copy()\n",
        "    y[idx] = np.clip(y[idx] + amp * rng.choice([-1.0, 1.0], size=k), -1.0, 1.0)\n",
        "    return y\n",
        "\n",
        "def _add_small_reverb(x, sr, rng, t_sec=0.03, decay=0.35, wet=0.18):\n",
        "    n = len(x)\n",
        "    if n == 0: return x\n",
        "    ir_len = max(8, int(t_sec * sr))\n",
        "    t = np.arange(ir_len, dtype=np.float32)\n",
        "    ir = np.exp(-decay * t / ir_len).astype(np.float32)\n",
        "    for _ in range(3):\n",
        "        pos = int(rng.integers(0, ir_len))\n",
        "        ir[pos] += float(rng.uniform(0.1, 0.3))\n",
        "    ir /= (np.sum(np.abs(ir)) + 1e-9)\n",
        "    y = np.convolve(x, ir, mode=\"full\")[:n].astype(np.float32)\n",
        "    return np.clip((1.0 - wet) * x + wet * y, -1.0, 1.0)\n",
        "\n",
        "# knobs copied from your MMS-TTS config\n",
        "RB_ENABLE           = True\n",
        "RB_BANDPASS_HZ      = (2500, 6000)\n",
        "RB_SNR_DB           = 25.0\n",
        "RB_NOISE_PROB       = 1.0\n",
        "RB_IMPULSE_PROB     = 0.5\n",
        "RB_IMPULSES_PER_SEC = 1.0\n",
        "RB_IMPULSE_GAIN     = 0.08\n",
        "RB_REVERB_PROB      = 0.6\n",
        "RB_REVERB_T_SEC     = 0.03\n",
        "RB_REVERB_DECAY     = 0.35\n",
        "RB_REVERB_WET       = 0.18\n",
        "\n",
        "def rawboost_v3(x: np.ndarray, sr: int, key: str) -> np.ndarray:\n",
        "    if x.ndim > 1: x = x.mean(axis=1)\n",
        "    x = np.clip(x, -1.0, 1.0).astype(np.float32)\n",
        "    if not RB_ENABLE: return x\n",
        "    rng = _rng_from_key(key)\n",
        "    if rng.uniform() < RB_NOISE_PROB:\n",
        "        x = _add_colored_noise(x, sr, rng, RB_BANDPASS_HZ, RB_SNR_DB)\n",
        "    if rng.uniform() < RB_IMPULSE_PROB:\n",
        "        x = _add_impulses(x, sr, rng, RB_IMPULSES_PER_SEC, RB_IMPULSE_GAIN)\n",
        "    if rng.uniform() < RB_REVERB_PROB:\n",
        "        x = _add_small_reverb(x, sr, rng, RB_REVERB_T_SEC, RB_REVERB_DECAY, RB_REVERB_WET)\n",
        "    return x\n",
        "\n",
        "def build_filter_chain_strong(gain_db: float) -> str:\n",
        "    return \",\".join([\n",
        "        \"highpass=f=20\",\n",
        "        \"equalizer=f=3000:t=q:w=1.0:g=2.5\",\n",
        "        \"equalizer=f=4800:t=q:w=0.9:g=2.0\",\n",
        "        \"treble=g=1.0:f=6000:t=h:w=0.7\",\n",
        "        f\"volume={gain_db}dB\",\n",
        "        f\"silenceremove=start_periods=1:start_duration={TRIM_DUR_S}:start_threshold={TRIM_THR_DB}dB:\"\n",
        "        f\"stop_periods=1:stop_duration={TRIM_DUR_S}:stop_threshold={TRIM_THR_DB}dB\"\n",
        "    ])\n",
        "\n",
        "# ------------ load inputs (FLEURS ONLY) ------------\n",
        "df_fl = read_csv_safe(FLEURS_CSV)\n",
        "df_fl = df_fl[df_fl[\"path\"].astype(str).apply(lambda p: Path(p).exists())].copy()\n",
        "df_in = df_fl.copy()\n",
        "\n",
        "# ------------ manifest / resume ------------\n",
        "MANIFEST.parent.mkdir(parents=True, exist_ok=True)\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST, usecols=[\"profile\",\"path_out\"])\n",
        "    done_set = set(mf.loc[mf[\"profile\"]==PROFILE, \"path_out\"].astype(str))\n",
        "else:\n",
        "    with open(MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "    done_set = set()\n",
        "\n",
        "def already_done(pin: str) -> bool:\n",
        "    pout = out_path_for(pin)\n",
        "    return pout.exists() or (str(pout) in done_set)\n",
        "\n",
        "todo_rows = [r for _, r in df_in.iterrows() if not already_done(str(r[\"path\"]))]\n",
        "planned, todo = len(df_in), len(todo_rows)\n",
        "print(f\"[preproc:{PROFILE}:FLEURS] PLANNED={planned:,} | ALREADY_DONE={planned-todo:,} | TO_DO={todo:,}\")\n",
        "\n",
        "# ------------ worker ------------\n",
        "def preprocess_one(row):\n",
        "    pin = str(row[\"path\"])\n",
        "    pout = out_path_for(pin)\n",
        "    ensure_parent(pout)\n",
        "\n",
        "    # read + resample if needed\n",
        "    try:\n",
        "        x, sr = sf.read(pin, dtype=\"float32\", always_2d=False)\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"read_fail: {e}\", \"path_in\": pin}\n",
        "    if hasattr(x, \"ndim\") and x.ndim > 1: x = x.mean(axis=1)\n",
        "    if sr != 16000:\n",
        "        tmp = pout.parent / f\".{pout.stem}.in-{uuid.uuid4().hex}.wav\"\n",
        "        cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "               \"-i\", pin, \"-ac\",\"1\",\"-ar\",\"16000\",\"-sample_fmt\",\"s16\", str(tmp)]\n",
        "        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        if p.returncode != 0:\n",
        "            try: tmp.unlink(missing_ok=True)\n",
        "            except: pass\n",
        "            return {\"error\": \"resample_fail\", \"path_in\": pin}\n",
        "        x, sr = sf.read(str(tmp), dtype=\"float32\", always_2d=False)\n",
        "        try: tmp.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "\n",
        "    x = np.clip(x, -1.0, 1.0).astype(np.float32)\n",
        "\n",
        "    # RawBoost v3–style\n",
        "    key = f\"{pin}|{PROFILE}\"\n",
        "    try:    y = rawboost_v3(x, sr, key)\n",
        "    except: y = x\n",
        "\n",
        "    # write RB temp near target (avoid cross-device link)\n",
        "    tmp_rb = pout.parent / f\".{pout.stem}.rb-{uuid.uuid4().hex}.wav\"\n",
        "    sf.write(str(tmp_rb), y, 16000, subtype=\"PCM_16\")\n",
        "\n",
        "    # EQ + loudness + trim via ffmpeg\n",
        "    gain_db = float(np.clip(TARGET_DB - rms_dbfs_arr(y), -20.0, 20.0))\n",
        "    FILTER  = build_filter_chain_strong(gain_db)\n",
        "\n",
        "    tmp_out = pout.parent / f\".{pout.stem}.tmp-{uuid.uuid4().hex}.wav\"\n",
        "    cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "           \"-i\", str(tmp_rb), \"-ac\",\"1\",\"-ar\",\"16000\",\n",
        "           \"-af\", FILTER, \"-sample_fmt\",\"s16\", str(tmp_out)]\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    try: tmp_rb.unlink(missing_ok=True)\n",
        "    except: pass\n",
        "    if p.returncode != 0:\n",
        "        try: tmp_out.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "        return {\"error\": p.stderr.decode(\"utf-8\",\"ignore\")[:300], \"path_in\": pin}\n",
        "\n",
        "    os.replace(str(tmp_out), str(pout))\n",
        "\n",
        "    # stats\n",
        "    try:\n",
        "        z, sr2 = sf.read(str(pout), dtype=\"float32\", always_2d=False)\n",
        "        if hasattr(z, \"ndim\") and z.ndim > 1: z = z.mean(axis=1)\n",
        "        dur = len(z)/float(sr2) if sr2 else 0.0\n",
        "        peak = float(np.max(np.abs(z))) if len(z) else 0.0\n",
        "        rms  = float(np.sqrt(np.mean(np.square(z)))) if len(z) else 0.0\n",
        "        rms_db = -120.0 if rms<=1e-9 else 20*np.log10(np.clip(rms,1e-9,1.0))\n",
        "        peak_db = -120.0 if peak<=1e-9 else 20*np.log10(np.clip(peak,1e-9,1.0))\n",
        "    except Exception:\n",
        "        dur, rms_db, peak_db = 0.0, -120.0, -120.0\n",
        "\n",
        "    return {\"utt_id\": row.get(\"utt_id\",\"\"), \"path_in\": pin, \"path_out\": str(pout),\n",
        "            \"profile\": PROFILE, \"duration\": round(dur,3),\n",
        "            \"rms_db\": round(rms_db,2), \"peak_db\": round(peak_db,2)}\n",
        "\n",
        "# ------------ scheduler + progress ------------\n",
        "created = 0; errs = 0; buf = []\n",
        "start = time.time()\n",
        "inflight_cap = max(MAX_WORKERS * INFLIGHT_MULT, MAX_WORKERS)\n",
        "pbar = tqdm(total=len(todo_rows), desc=f\"preproc:{PROFILE}:FLEURS\", unit=\"file\")\n",
        "\n",
        "def flush_manifest(rows):\n",
        "    if not rows: return\n",
        "    with open(MANIFEST, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
        "        w = csv.DictWriter(fcsv, fieldnames=[\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "        for r in rows: w.writerow(r)\n",
        "\n",
        "it = iter(todo_rows)\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    inflight = set()\n",
        "    for _ in range(min(inflight_cap, len(todo_rows))):\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: break\n",
        "    while inflight:\n",
        "        done = next(as_completed(inflight))\n",
        "        inflight.remove(done)\n",
        "        res = done.result()\n",
        "        pbar.update(1)\n",
        "        if isinstance(res, dict) and \"error\" in res:\n",
        "            errs += 1\n",
        "        elif res:\n",
        "            buf.append(res); created += 1\n",
        "            if len(buf) >= BATCH_WRITE:\n",
        "                flush_manifest(buf); buf.clear()\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: pass\n",
        "\n",
        "flush_manifest(buf)\n",
        "pbar.close()\n",
        "print(f\"\\n[{PROFILE}:FLEURS] FINISHED: +{created} files, {errs} errors | elapsed {(time.time()-start)/60:.1f} min\")\n",
        "\n",
        "# quick manifest slice\n",
        "if MANIFEST.exists():\n",
        "    m = pd.read_csv(MANIFEST)\n",
        "    mp = m[m[\"profile\"]==PROFILE]\n",
        "    hrs = float(pd.to_numeric(mp[\"duration\"], errors=\"coerce\").fillna(0).sum()/3600.0)\n",
        "    print(f\"[{PROFILE}] Manifest rows: {len(mp):,} | hours: {hrs:.2f} h\")\n",
        "\n",
        "print(\"[OK] STRONG preprocessing for FLEURS done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "609d34d600b04a92b4ff2f812dff3b8a",
            "0eea2b89565d4f78a78759477fcdb06c",
            "a6f0f222ae254a959765984eb0d7bb0f",
            "25fc06ea0c22453398d89623fa937d8b",
            "ecf3e9e8f8d6481c82d85250d6f5bcab",
            "2acbd15ae3f444139336a6fc73b23474",
            "af663a45205b40659d3991f6833cbe15",
            "00fd2d4880d94c18aa099f4eb0715f29",
            "415f040effc74dda8a0101c6ce87b202",
            "9e80cbc9973048cab0c9cd2bbecd1b3a",
            "b1edb1480d4c4a678b9bfcff1228b4e5"
          ]
        },
        "id": "NT5FYyAsErBu",
        "outputId": "1020ae34-c3e5-4bc7-ffef-e041e33383a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[preproc:strong:FLEURS] PLANNED=604 | ALREADY_DONE=0 | TO_DO=604\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preproc:strong:FLEURS:   0%|          | 0/604 [00:00<?, ?file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "609d34d600b04a92b4ff2f812dff3b8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[strong:FLEURS] FINISHED: +604 files, 0 errors | elapsed 2.3 min\n",
            "[strong] Manifest rows: 98,397 | hours: 186.23 h\n",
            "[OK] STRONG preprocessing for FLEURS done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **11. Profile: Clean - Test Set**\n",
        "We generate a separate test set using the `clean` profile. This profile applies **Artifact EQ** and normalization but **omits RawBoost** (noise/reverb). This provides a baseline for evaluating model performance on high-quality, non-degraded audio."
      ],
      "metadata": {
        "id": "P8uF_MgdNCNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= PREPROCESSING — CLEAN (for test set) =================\n",
        "# Writes: /content/drive/MyDrive/hindi_dfake/processed/wav/clean/...\n",
        "# Logs to: /content/drive/MyDrive/hindi_dfake/metadata/proc_manifest_clean.csv\n",
        "\n",
        "import os, csv, uuid, time, subprocess\n",
        "from pathlib import Path\n",
        "import pandas as pd, numpy as np, soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ------------ CONFIG ------------\n",
        "ROOT = Path(\"/content/drive/MyDrive/hindi_dfake\")\n",
        "META = ROOT / \"metadata\"\n",
        "OUT_ROOT = ROOT / \"processed\" / \"wav\"\n",
        "\n",
        "PROFILE = \"clean\"  # clean = NO RawBoost, just EQ + normalize + trim\n",
        "TARGET_DB   = -26.0\n",
        "TRIM_THR_DB = -45\n",
        "TRIM_DUR_S  = 0.20\n",
        "\n",
        "# Concurrency\n",
        "MAX_WORKERS   = 6\n",
        "INFLIGHT_MULT = 3\n",
        "BATCH_WRITE   = 500\n",
        "\n",
        "# Test set CSVs (both real and fake)\n",
        "TEST_REAL_CSV = META / \"fs_test_real.labeled.csv\"\n",
        "TEST_FAKE_CSV = META / \"fs_test_fake_mms.labeled.csv\"\n",
        "MANIFEST = META / \"proc_manifest_clean.csv\"\n",
        "\n",
        "assert TEST_REAL_CSV.exists(), f\"Missing {TEST_REAL_CSV}\"\n",
        "assert TEST_FAKE_CSV.exists(), f\"Missing {TEST_FAKE_CSV}\"\n",
        "\n",
        "# ------------ helpers ------------\n",
        "def read_csv_safe(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    for c in [\"utt_id\",\"path_audio\",\"duration\",\"label\"]:\n",
        "        if c not in df.columns:\n",
        "            df[c] = \"\" if c != \"duration\" else 0.0\n",
        "    return df\n",
        "\n",
        "def infer_raw_path(processed_path: str) -> str:\n",
        "    \"\"\"\n",
        "    STRONG-only mapping:\n",
        "      <anything>/hindi_dfake/processed/wav/strong/raw/<suffix>\n",
        "    → /content/drive/MyDrive/hindi_dfake/raw/<suffix>\n",
        "\n",
        "    Works even if the CSV contains Windows-style paths like:\n",
        "      G:\\\\My Drive\\\\hindi_dfake\\\\processed\\\\wav\\\\strong\\\\raw\\\\...\n",
        "    \"\"\"\n",
        "    s = str(processed_path).replace(\"\\\\\", \"/\")\n",
        "    marker = \"hindi_dfake/processed/wav/strong/raw/\"\n",
        "\n",
        "    if marker not in s:\n",
        "        return str(processed_path)\n",
        "\n",
        "    suffix = s.split(marker, 1)[1]  # e.g. \"real_clean/ivr/xxx.wav\"\n",
        "    return str((ROOT / \"raw\" / suffix).resolve())\n",
        "\n",
        "def out_path_for(raw_path: str) -> Path:\n",
        "    \"\"\"Map raw audio path to clean output path\"\"\"\n",
        "    p = Path(raw_path)\n",
        "    try:\n",
        "        rel = p.relative_to(ROOT / \"raw\")\n",
        "    except:\n",
        "        rel = Path(\"_external\") / p.name\n",
        "    return OUT_ROOT / PROFILE / rel\n",
        "\n",
        "def ensure_parent(p: Path):\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def rms_dbfs_arr(x: np.ndarray) -> float:\n",
        "    if x.ndim > 1:\n",
        "        x = x.mean(axis=1)\n",
        "    if len(x) == 0:\n",
        "        return -120.0\n",
        "    rms = float(np.sqrt(np.mean(np.square(x))))\n",
        "    if rms <= 1e-9:\n",
        "        return -120.0\n",
        "    return 20.0*np.log10(np.clip(rms, 1e-9, 1.0))\n",
        "\n",
        "def build_filter_chain_clean(gain_db: float) -> str:\n",
        "    \"\"\"Same EQ as strong, but NO RawBoost beforehand\"\"\"\n",
        "    return \",\".join([\n",
        "        \"highpass=f=20\",\n",
        "        \"equalizer=f=3000:t=q:w=1.0:g=2.5\",\n",
        "        \"equalizer=f=4800:t=q:w=0.9:g=2.0\",\n",
        "        \"treble=g=1.0:f=6000:t=h:w=0.7\",\n",
        "        f\"volume={gain_db}dB\",\n",
        "        f\"silenceremove=start_periods=1:start_duration={TRIM_DUR_S}:start_threshold={TRIM_THR_DB}dB:stop_periods=1:stop_duration={TRIM_DUR_S}:stop_threshold={TRIM_THR_DB}dB\",\n",
        "    ])\n",
        "\n",
        "# ------------ load test set ------------\n",
        "df_real = read_csv_safe(TEST_REAL_CSV)\n",
        "df_fake = read_csv_safe(TEST_FAKE_CSV)\n",
        "\n",
        "# Convert path_audio (processed/strong) to raw paths (STRONG-only)\n",
        "df_real[\"path_raw\"] = df_real[\"path_audio\"].astype(str).apply(infer_raw_path)\n",
        "df_fake[\"path_raw\"] = df_fake[\"path_audio\"].astype(str).apply(infer_raw_path)\n",
        "\n",
        "# Debug sanity (recommended)\n",
        "print(\"Sample mapping (REAL):\")\n",
        "print(df_real[[\"path_audio\",\"path_raw\"]].head(3).to_string(index=False))\n",
        "print(\"Exists?:\", [Path(p).exists() for p in df_real[\"path_raw\"].head(3)])\n",
        "\n",
        "# Filter to existing files only\n",
        "df_real = df_real[df_real[\"path_raw\"].apply(lambda p: Path(p).exists())].copy()\n",
        "df_fake = df_fake[df_fake[\"path_raw\"].apply(lambda p: Path(p).exists())].copy()\n",
        "\n",
        "df_test = pd.concat([df_real, df_fake], ignore_index=True)\n",
        "\n",
        "print(f\"\\nTest set files found:\")\n",
        "print(f\"  Real: {len(df_real):,}\")\n",
        "print(f\"  Fake: {len(df_fake):,}\")\n",
        "print(f\"  Total: {len(df_test):,}\")\n",
        "\n",
        "# ------------ manifest skip logic ------------\n",
        "MANIFEST.parent.mkdir(parents=True, exist_ok=True)\n",
        "if MANIFEST.exists():\n",
        "    mf = pd.read_csv(MANIFEST, usecols=[\"profile\",\"path_out\"])\n",
        "    done_set = set(mf.loc[mf[\"profile\"] == PROFILE, \"path_out\"].astype(str))\n",
        "else:\n",
        "    with open(MANIFEST, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        csv.writer(f).writerow([\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "    done_set = set()\n",
        "\n",
        "def already_done(pin: str) -> bool:\n",
        "    pout = out_path_for(pin)\n",
        "    return pout.exists() or (str(pout) in done_set)\n",
        "\n",
        "todo_rows = [r for _, r in df_test.iterrows() if not already_done(str(r[\"path_raw\"]))]\n",
        "planned, todo = len(df_test), len(todo_rows)\n",
        "print(f\"\\n[preproc:{PROFILE}] PLANNED={planned:,} | ALREADY_DONE={planned-todo:,} | TO_DO={todo:,}\")\n",
        "\n",
        "# ------------ worker (NO RAWBOOST) ------------\n",
        "def preprocess_one(row):\n",
        "    pin = str(row[\"path_raw\"])\n",
        "    pout = out_path_for(pin)\n",
        "    ensure_parent(pout)\n",
        "\n",
        "    # Read and resample to 16k mono\n",
        "    try:\n",
        "        x, sr = sf.read(pin, dtype=\"float32\", always_2d=False)\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"read_fail: {e}\", \"path_in\": pin}\n",
        "\n",
        "    if x.ndim > 1:\n",
        "        x = x.mean(axis=1)\n",
        "\n",
        "    if sr != 16000:\n",
        "        tmp = pout.parent / f\".{pout.stem}.in-{uuid.uuid4().hex}.wav\"\n",
        "        cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "               \"-i\", pin, \"-ac\",\"1\",\"-ar\",\"16000\",\"-sample_fmt\",\"s16\", str(tmp)]\n",
        "        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        if p.returncode != 0:\n",
        "            try: tmp.unlink(missing_ok=True)\n",
        "            except: pass\n",
        "            return {\"error\": \"resample_fail\", \"path_in\": pin}\n",
        "        x, sr = sf.read(str(tmp), dtype=\"float32\", always_2d=False)\n",
        "        try: tmp.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "\n",
        "    x = np.clip(x, -1.0, 1.0).astype(np.float32)\n",
        "\n",
        "    # NO RawBoost here\n",
        "    tmp_clean = pout.parent / f\".{pout.stem}.clean-{uuid.uuid4().hex}.wav\"\n",
        "    sf.write(str(tmp_clean), x, 16000, subtype=\"PCM_16\")\n",
        "\n",
        "    # Compute gain and apply EQ + normalize + trim\n",
        "    gain_db = float(np.clip(TARGET_DB - rms_dbfs_arr(x), -20.0, 20.0))\n",
        "    FILTER = build_filter_chain_clean(gain_db)\n",
        "\n",
        "    tmp_out = pout.parent / f\".{pout.stem}.tmp-{uuid.uuid4().hex}.wav\"\n",
        "    cmd = [\"ffmpeg\",\"-nostdin\",\"-hide_banner\",\"-loglevel\",\"error\",\"-y\",\n",
        "           \"-i\", str(tmp_clean), \"-ac\",\"1\",\"-ar\",\"16000\",\n",
        "           \"-af\", FILTER, \"-sample_fmt\",\"s16\", str(tmp_out)]\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    try: tmp_clean.unlink(missing_ok=True)\n",
        "    except: pass\n",
        "\n",
        "    if p.returncode != 0:\n",
        "        try: tmp_out.unlink(missing_ok=True)\n",
        "        except: pass\n",
        "        return {\"error\": p.stderr.decode(\"utf-8\",\"ignore\")[:300], \"path_in\": pin}\n",
        "\n",
        "    os.replace(str(tmp_out), str(pout))\n",
        "\n",
        "    # Stats\n",
        "    try:\n",
        "        z, sr2 = sf.read(str(pout), dtype=\"float32\", always_2d=False)\n",
        "        if z.ndim > 1:\n",
        "            z = z.mean(axis=1)\n",
        "        dur = len(z)/float(sr2) if sr2 else 0.0\n",
        "        peak = float(np.max(np.abs(z))) if len(z) else 0.0\n",
        "        rms = float(np.sqrt(np.mean(np.square(z)))) if len(z) else 0.0\n",
        "        rms_db = -120.0 if rms<=1e-9 else 20*np.log10(np.clip(rms,1e-9,1.0))\n",
        "        peak_db = -120.0 if peak<=1e-9 else 20*np.log10(np.clip(peak,1e-9,1.0))\n",
        "    except Exception:\n",
        "        dur, rms_db, peak_db = 0.0, -120.0, -120.0\n",
        "\n",
        "    return {\n",
        "        \"utt_id\": row.get(\"utt_id\",\"\"),\n",
        "        \"path_in\": pin,\n",
        "        \"path_out\": str(pout),\n",
        "        \"profile\": PROFILE,\n",
        "        \"duration\": round(dur,3),\n",
        "        \"rms_db\": round(rms_db,2),\n",
        "        \"peak_db\": round(peak_db,2),\n",
        "    }\n",
        "\n",
        "# ------------ process ------------\n",
        "created = 0; errs = 0; buf = []\n",
        "start = time.time()\n",
        "inflight_cap = max(MAX_WORKERS * INFLIGHT_MULT, MAX_WORKERS)\n",
        "pbar = tqdm(total=len(todo_rows), desc=f\"preproc:{PROFILE}\", unit=\"file\")\n",
        "\n",
        "def flush_manifest(rows):\n",
        "    if not rows: return\n",
        "    with open(MANIFEST, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
        "        w = csv.DictWriter(fcsv, fieldnames=[\"utt_id\",\"path_in\",\"path_out\",\"profile\",\"duration\",\"rms_db\",\"peak_db\"])\n",
        "        for r in rows:\n",
        "            w.writerow(r)\n",
        "\n",
        "it = iter(todo_rows)\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "    inflight = set()\n",
        "    for _ in range(min(inflight_cap, len(todo_rows))):\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: break\n",
        "\n",
        "    while inflight:\n",
        "        done = next(as_completed(inflight))\n",
        "        inflight.remove(done)\n",
        "        res = done.result()\n",
        "        pbar.update(1)\n",
        "\n",
        "        if isinstance(res, dict) and \"error\" in res:\n",
        "            errs += 1\n",
        "        elif res:\n",
        "            buf.append(res); created += 1\n",
        "            if len(buf) >= BATCH_WRITE:\n",
        "                flush_manifest(buf); buf.clear()\n",
        "\n",
        "        try: inflight.add(ex.submit(preprocess_one, next(it)))\n",
        "        except StopIteration: pass\n",
        "\n",
        "flush_manifest(buf)\n",
        "pbar.close()\n",
        "print(f\"\\n[{PROFILE}] FINISHED: +{created} files, {errs} errors | elapsed {(time.time()-start)/60:.1f} min\")\n",
        "\n",
        "# Summary\n",
        "if MANIFEST.exists():\n",
        "    m = pd.read_csv(MANIFEST)\n",
        "    mp = m[m[\"profile\"] == PROFILE]\n",
        "    hrs = float(pd.to_numeric(mp[\"duration\"], errors=\"coerce\").fillna(0).sum()/3600.0)\n",
        "    print(f\"[{PROFILE}] Manifest rows: {len(mp):,} | hours: {hrs:.2f} h\")\n",
        "\n",
        "    print(f\"\\n[{PROFILE}] Output directories created:\")\n",
        "    for parent in sorted(set(Path(p).parent for p in mp[\"path_out\"])):\n",
        "        count = sum(1 for p in mp[\"path_out\"] if str(p).startswith(str(parent)))\n",
        "        print(f\"  {parent} -> {count} files\")\n",
        "\n",
        "print(\"\\n[OK] CLEAN test set preprocessing done.\")\n",
        "print(f\"Output location: {OUT_ROOT / PROFILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503,
          "referenced_widgets": [
            "f8c5e6fd3dd94afa902125faa9b97d0e",
            "a6db3f4e046e46e8887e3fc49b71b98b",
            "0fc8c5375dd0410982b9c65b9144ffbb",
            "890c98a273314a55a66de5e1d48c631c",
            "04395910b6934f34a48b763f3697cdc2",
            "64fb76f939ee475d835e03298496ce2e",
            "03addb8ba5764cfb8536abafeea5923f",
            "9fdb003aa9f543848bb860f455f6a824",
            "05bb3beee5dc47c9ba2fddbbbb645749",
            "e29ac2a1159f4084a13623df0245ce88",
            "3a245bf036464a0184b9c224004f853e"
          ]
        },
        "id": "tvexBSpKRu1k",
        "outputId": "e17560d8-86e5-4e76-c499-726f9e60ab8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample mapping (REAL):\n",
            "                                                                                                        path_audio                                                                                                 path_raw\n",
            "G:\\My Drive\\hindi_dfake\\processed\\wav\\strong\\raw\\real_clean\\thirdparty\\fleurs_hi_in\\fleurs_hi_8b63ce7acd912273.wav /content/drive/MyDrive/hindi_dfake/raw/real_clean/thirdparty/fleurs_hi_in/fleurs_hi_8b63ce7acd912273.wav\n",
            "G:\\My Drive\\hindi_dfake\\processed\\wav\\strong\\raw\\real_clean\\thirdparty\\fleurs_hi_in\\fleurs_hi_6c47be90259fefd1.wav /content/drive/MyDrive/hindi_dfake/raw/real_clean/thirdparty/fleurs_hi_in/fleurs_hi_6c47be90259fefd1.wav\n",
            "G:\\My Drive\\hindi_dfake\\processed\\wav\\strong\\raw\\real_clean\\thirdparty\\fleurs_hi_in\\fleurs_hi_cb0357a131aedb4f.wav /content/drive/MyDrive/hindi_dfake/raw/real_clean/thirdparty/fleurs_hi_in/fleurs_hi_cb0357a131aedb4f.wav\n",
            "Exists?: [True, True, True]\n",
            "\n",
            "Test set files found:\n",
            "  Real: 3,022\n",
            "  Fake: 3,002\n",
            "  Total: 6,024\n",
            "\n",
            "[preproc:clean] PLANNED=6,024 | ALREADY_DONE=430 | TO_DO=5,594\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preproc:clean:   0%|          | 0/5594 [00:00<?, ?file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8c5e6fd3dd94afa902125faa9b97d0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[clean] FINISHED: +5594 files, 0 errors | elapsed 17.7 min\n",
            "[clean] Manifest rows: 5,594 | hours: 8.11 h\n",
            "\n",
            "[clean] Output directories created:\n",
            "  /content/drive/MyDrive/hindi_dfake/processed/wav/clean/fake_tts_mms -> 3002 files\n",
            "  /content/drive/MyDrive/hindi_dfake/processed/wav/clean/real_clean/commonvoice -> 468 files\n",
            "  /content/drive/MyDrive/hindi_dfake/processed/wav/clean/real_clean/ivr -> 1950 files\n",
            "  /content/drive/MyDrive/hindi_dfake/processed/wav/clean/real_clean/thirdparty/fleurs_hi_in -> 174 files\n",
            "\n",
            "[OK] CLEAN test set preprocessing done.\n",
            "Output location: /content/drive/MyDrive/hindi_dfake/processed/wav/clean\n"
          ]
        }
      ]
    }
  ]
}